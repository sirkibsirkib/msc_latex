
\chapter{Background}
\section{Reo}
\label{sec:reo_background}
Reo is a high-level language for specifying protocols. Here, we explore the motivation behind Reo's development, how the language is used, and (at high level) how it works. 
The Reo language has applicability whenever there is a benefit in being able to formalize a communication protocol. However, this work primarily focuses on Reo's role in automatic generation of glue-code for applications.  

\subsection{Motivation}
\label{sec:reo_motivation}
TODO focus on safety properties

Modern software development involves the construction of large and complex projects. Owing to their scale and the heterogeneity of the tasks required, many people are involved in the development of a program at once, and over its development lifetime. The industry has long-since established paradigms for managing the scale of these projects. One tenet of good software design is \textit{modularity}, which describes a structure that, instead of being designed monolithically, is built out of smaller constituent modules. In addition to isolating modules such that they can be re-used in other projects, this design philosophy allows contributors to concentrate on a subset of all the modules at a time. These ideas are well-established in practice; code re-use and separation-of-concerns have been prevalent for some time.

Reo's utility is not only its ability to facilitate modularity. Reo is designed such that properties of the individual modules are \textit{preserved} when modules are \textit{composed} into larger ones. This preservation marks the difference between \textit{gluing} modules together (and hoping for the best), and \textit{composing} them into something guaranteed to have the intended properties. 

\subsection{Language}
\label{sec:reo_lang}
Reo is a \textit{coordination} language. This describes its focus on the specification of the \textit{interactions} between distinct actors. This is in contrast to the usual \textit{action-centric} model common to languages with their roots in sequential programming, where the programs or specifications describe \textit{actions} of entities, relegating any associated interactions to requiring derivation from the actions. In a nutshell, Reo provides a language for describing the behavior of a \textit{system} of actors by explicitly constraining the behavior of the \textit{connector} which serves as their communication medium. 

(TODO define connector. same as component just maybe structural?)

The Reo language is essentially graphic; each connector defines a relation over named \textit{locations}. Complex connectors are defined as the composition of simpler connectors over its locations. This inherently visual language is also often seen in its textual form, usually in the context of machine parsing. 

The simplest \textit{primitive} connectors cannot be subdivided

by listing a set of constituent connectors. The simplest primitive Reo connectors are \textit{channels}(TODO channel vs primitive). Channels by definition cannot be subdivided into constituent connectors, as they are defined by either (a) the model that provides Reo's semantics, or (b) opaque components defined in some target language such as C or Java. The nodes themselves are the other important aspect of the language. Ultimately, each node corresponds to a (logical) location which may hold up to one datum at a time. Reo is by default \textit{synchronous}, and relationships between locations propagates that synchrony. \textit{Locations} are divided into two classes according to whether 

(TODO)

This motivates the Reo's metaphor of propagation of data and back-propagation of data-constraints, corresponding to its namesake, the Greek word for `flow'. The compositional aspect is meaningful when locations are involved in multiple relationships.

\textit{forwards} (by moving data several `hops' at once) and backwards

In addition to re-using nodes inside a connector, connectors are able to expose these nodes for re-use in the connectors \textit{above them} by exposing the node in the connector's \textit{interface}. These exposed nodes are called \textit{ports}, leaning on the metaphor of the connector \textit{moving} some data in and out of itself.

(data flow corresponds with what happens at runtime, except its SYNCHRONOUS by default. Relate to TDS. Talk about replication and equality checks).

(In the context of applications, components that cannot be composed compile to things managed by different threads. at the boundaries, they communicate with ports. Here, there is a meaningful difference between putter and getter. Include example of how a protocol that uses some port A three times still results in a putter-getter pair)


\subsection{Typical Channels}

In principle, Reo does not enforce the use of any primitive channels in particular. Users are free to use channels that are best-suited to their use case. In practice, a small set of exceptionally simple channels are favoured in literature and in practice owing to their versatility. As such, this work presumes that these consititute the majority of the channels out of which our protocols are composed. Below, we enumerate this set of channels and their behavior.

\begin{enumerate}
	\item sync($I_0, O_0$)
	\item fifo1($I_0, O_0$)
	\item lossy\_sync($I_0, O_0$)
	\item exclusive\_router($I_0, O_0, O_1$)
\end{enumerate}


\subsection{Semantic Models}
\label{sec:semantic_models}
Reo took a number of years to take its present shape. It is recognizable as early as 2001, but was presented as a concept before it was formalized, leaving it as a task for future work~\cite{jongmans2012overview}. Later, This several different approaches to formal semantics were developed. For our purposes, it suffices to concentrate only on the small subset of the semantics to follow. For additional information, the work of Jongmans in particular serves as a good entry point\cite{jongmans2012overview}.


Starting with the fundamentals, a \textbf{stream} specifies the value of a variable from data domain $D$ changing over the course of a sequence of events. Usually streams are considered infinite, and so it is practical to define them as a function $\mathbb{N}\mapsto{}D$. A \textbf{timed data stream} (TDS) takes this notion a step further, annotating each event in the sequence with an increasing \textit{time stamp}. A TDS is defined by some tuple $(\mathbb{N}\mapsto{}\mathbb{R}, \mathbb{N}\mapsto{}D)$, or equivalently, $\mathbb{N}\mapsto{}(\mathbb{R}, D)$ with the added constraints that time must increase toward infinity\cite{arbab2004modeling}. By associating one TDS with each \textit{named variable} of a program, one can represent a \textit{trace} of its execution. TDS events with the same time stamp are considered simultaneous, allowing reasoning about \textit{snapshots} of the program's state over its run time. These traces can be practically visualized as \textbf{trace tables}, with variables for columns and time stamps for rows by representing the absence of data observations using a special `silent' symbol \textbf{*}, referring \textit{silent behavior}. In this work, we use `trace tables' to refer to both the visualization and to a program trace as a set of named TDS's. The runs of finite programs can be simulated either by bounding the tables (constraining the TDS domain to be finite), or by simulating finite behavior as infinite by extending the `end' forevermore with silent behavior. Table~\ref{tab:fifo1_eg} gives an example of a trace table for some program with two named variables.


\begin{table}[]
	\centering
	\begin{tabular}{l|cc}
		$\mathbb{R}$  & A & B \\ \hline
		0.0 & 0 & * \\
		0.1 & * & 0 \\
		0.2 & * & * \\
		0.3 & 1 & * \\
		0.4 & * & 1
	\end{tabular}
	\caption[Trace table of a system adherent to fifo1.]{Trace table comprised of TDS's for variables $A$ and $B$. This trace represents behavior that adheres to the \textit{fifo1} protocol with input and output ports $A$ and $B$ respectively.}
	\label{tab:fifo1_eg}
\end{table}


One of it's earlier \textit{coalegebraic models} represented Reo connectors as \textbf{stream constraints} (SC) over such TDS tables in which variables are ports~\cite{arbab2004reo}. Here, constraints are usually defined in first-order \textit{temporal logic}, which allows the discrimination of streams according to their values both now and arbitrarily far into the future\footnote{Not all variants of temporal logic are equally (succinctly) expressive. It requires a notion of `bounded lookahead' to express a notion such as `$P$ holds for the next 3 states' as something like $\square ^{1-3} P$ rather than the verbose $(\square P \wedge \square \square P \wedge \square \square \square P)$.}. This model is well-suited for translating from the kinds of safety properties that are typically desired in practice. Statements such as `$A$ never receives a message before $B$ has communicated with $C$' have clear mappings to temporal logic, as often it is intuitive to reason about safety by reasoning about future events. Table~\ref{tab:fifo1_eg} above shows the trace of a program that adheres the \textit{fifo1} protocol with ports $A$ and $B$ as input and output respectively.

SC are unwieldy in the context of code generation. In reality, it is easier to predicate one's next actions as a function of the \textit{past} rather than the future. Accordingly, \textbf{constraint automata} (CA) was one of the \textit{operational models} for modeling Reo connectors that has a clearer correspondence to stateful computation. Where an NFA accepts finite strings, a CA accepts trace tables. Thus, each CA represents some protocol. Programs are adherent to the protocol if and only if it always generates only accepted trace tables. From an implementation perspective, CA can be thought to enumerate precisely the actions which are allowed at ports given the correct states, and prohibiting everything else by default. A CA is defined with a state set and initial state as usual, but each transition is given \textit{constraints} that prevent their firing unless satisfied; each transition has both (a) the \textit{synchronization constraint}, the set of ports which perform actions, and (b) a \textit{data constraint} predicate over the values of ports in the firing set at the `current' time step. For example, Listing~\ref{tab:fifo1_eg} above is accepted by the CA of the \textit{fifo1} connector with all ports of binary data type $\{0,1\}$. Observe that here the automaton discriminates the previously-buffered value (`remembering' what $A$ stored) by distinguishing the options with states $q_{f0}$ and $q_{f1}$. As a consequence, it is not possible to represent a \textit{fifo1} protocol for an infinite data domain without requiring infinite states.
\begin{figure}[ht]
	\centering
	\begin{tikzpicture}
	\node[state, initial above] (q0) {$q_{e}$};
	\node[state, left of=q0] (q1) {$q_{f0}$};
	\node[state, right of=q0] (q2) {$q_{f1}$};
	\draw
	(q0) edge[bend left, below] node{$\{A\}$\\$d_A$=0} (q1)
	(q1) edge[bend left, above] node{$\{B\}$\\\hspace{1em}$d_B$=0} (q0)
	
	(q0) edge[bend right, below] node{$\{A\}$\\$d_A$=1} (q2)
	(q2) edge[bend right, above] node{$\{B\}$\\\hspace{1em}$d_B$=1} (q0)
	;
	\end{tikzpicture}
	\caption[CA for fifo1 connector.]{CA for the \textit{fifo1} protocol with ports $A$ and $B$ sharing data domain $\{0,1\}$.}
	\label{fig:fifo1_ca}
\end{figure}

Later, CA were extended to include \textit{memory cells} (or \textit{memory variables}) which act as value stores whose contents \textit{persist} into the future. Data constraints are provided the ability to assign to their \textit{next} value, typically using syntax from temporal logic (eg: $m'$ is the value of $m$ at the next time stamp). Figure~\ref{fig:fifo1_ca_mem} revisits the \textit{fifo1} protocol from before. With this extension, the task of persistently storing $A$'s value into the buffer can be relegated to $m$, simplifying the state space significantly. This change also makes it possible to represent connectors for arbitrary data domains, finite or otherwise.



\begin{figure}[ht]
	\centering
	\begin{tikzpicture}
	\node[state, initial] (q0) {$q_{e}$};
	\node[state, right of=q0] (q1) {$q_{f}$};
	\draw
	(q0) edge[bend left, above] node{$\{A\}$\\$d_A=m'$} (q1)
	(q1) edge[bend left, below] node{$\{B\}$\\$d_B=m$} (q0)
	;
	\end{tikzpicture}
	\caption[CA with memory for fifo1 connector.]{CA with memory cell~$m$ for Reo connector~$fifo1$ with arbitrary data domain~$D$ common to ports~$A$ and~$B$. Two states are used to track to enforce alternation between filling and emptying~$m$.}
	\label{fig:fifo1_ca_mem}
\end{figure}


For the purposes of Reo, we are interested in being able to compute the composition of CAs to acquire a model for the compositions of their protocols. Figure~\ref{fig:fifo2_ca} shows an example of such a composition, producing \textit{fifo2} by composing \textit{fifo1} with itself. This new protocol indeed exhibits the desired behavior; the memory cells are able to store up to two elements at a time, and $B$ is guaranteed to consume values in the order that $A$ produced them. Even at this small scale, we see how the composition of such CA have a tendency to result in an \textit{explosion} if state- and transition-space. When seen at larger scales, a \textit{fifo$N$} buffer consists of $2^N$ states. The problem is the inability for a CA to perform any meaningful \textit{abstraction}; here, it manifests as the automaton having to express its transition system in undesired specificity. Intuitively, the contents of $m_0$ are irrelevant when $m_1$ is drained by $B$, but the CA requires two transitions to cover the possible cases in which this action is available. In the context of accepting existing trace tables, data constraints are evaluated predictably. However, in the case of code generation we are able to treat the data constraint instead as a pair of (a) the \textit{guard} which enables the transition as a function of the \textit{present} time stamp, and (b) the \textit{assignment}, which may reason about the next time step, and which we are able to guarantee by \textit{assigning} variables. As such, data constraints are broken up into these parts where possible. Figure~\ref{fig:fifo2_ca} and others to follow formulate their data constraints such that the guard and assignment parts are identifiable wherever it is practical to do so.


\begin{figure}[ht]
	\centering
	\begin{tikzpicture}
	\node[state, initial]      (qee) {$q_{ee}$};
	\node[state, right of=qee] (qfe) {$q_{fe}$};
	\node[state, below of=qfe] (qff) {$q_{ff}$};
	\node[state, below of=qee] (qef) {$q_{ef}$};
	\draw
	(qee) edge[above] node{$\{A\}$\\$m_0'=d_A$} (qfe)
	(qfe) edge[below] node[pos=0.43]{$\quad{}\{\}$\\$\quad{}m_1'=m_0$} (qef)
	(qff) edge[right] node{$\{B\}$\\$d_B=m_1$} (qfe)
	(qef) edge[below] node{$\{A\}$\\$m_0'=d_A$} (qff)
	(qef) edge[left] node{$\{B\}$\\$d_B=m_1$} (qee)
	;
	\end{tikzpicture}
	\caption[CA with memory for fifo2 connector.]{CA with memory cells $m_0$ and $m_1$ for the \textit{fifo2} connector with an arbitrary data domain for ports $A$ and $B$. Transitions are spread over the state space such that the automaton's structure results in the \textit{first-in-first-out} behavior of the memory cells in series.}
	\label{fig:fifo2_ca}
\end{figure}


\begin{figure}[ht]
	\centering
	\begin{tikzpicture}
	\node[state, initial] (q0) {$ $};
	\draw
	(q0) edge[loop above] node{$\{A\}$\\$m=*$\\$\gapwedge{}m'=d_A$} (q0)
	(q0) edge[loop below] node{$\{B\}$\\$m\neq{}*$\\$\gapwedge{}d_B=m\gapwedge{}m'=*$} (q0)
	;
	\end{tikzpicture}
	\caption[RBA for fifo1 connector.]{RBA of the \textit{fifo1} connector for an arbitrary data domain common to ports $A$ and $B$. Memory cell $m$ is used both to buffer $A$'s value, and as part of the data constraint on both transitions for \textit{emptying} and \textit{filling} the cell to ensure these interactions are always interleaved. Data constraints are formulated for readability such that the `guard' and `assignment' conjuncts are line-separated.}
	\label{fig:fifo1_rba}
\end{figure}
Evidently, memory cells provide a new means of enforcing how data persists over time. In many cases, it can be seen that the same connectors can be represented differently by moving this responsibility between state- and data-domains. \textbf{Rule-based automata} (RBA) are the cases of CA for which this idea is taken to an extreme by relying only on memory cells entirely; RBAs have only one state. Figure~\ref{fig:fifo1_rba} models the \textit{fifo1} connector once again, this time as an RBA. Aside from the added expressivity, RBAs benefit from being cheaper to compose. As the state space is degenerate, RBAs may be easily re-interpreted into forms more easy to work with. \textbf{Rule-based form} (RBF) embraces the statelessness of an RBA as a single formula, the \textit{disjunction} of its constraints. In this view, Dokter et al.\ defines their composition of connectors such that, instead of exploding, the composed connector has transitions and memory cells that are the \textit{sum} of its constituent connectors\cite{dokter2018rule}.



\begin{figure}[ht]
	\centering
	\begin{tikzpicture}
	\node[state, initial] (q0) {$ $};
	\draw
	(q0) edge[loop above] node{$\{A\}$\\$m_0=*$\\$\gapwedge{}m_0'=d_A$} (q0)
	(q0) edge[loop right] node{$\{\}$\\$m_0\neq{}*\gapwedge{}m_1=*$\\$\gapwedge{}m_1'=m_0\gapwedge{}m_0'=*$} (q0)
	(q0) edge[loop below] node{$\{B\}$\\$m_1\neq{}*$\\$\gapwedge{}d_B=m_1\gapwedge{}m_1'=*$} (q0)
	;
	\end{tikzpicture}
	\caption[RBA for fifo2 connector.]{RBA of the \textit{fifo2} connector for an arbitrary data domain common to ports $A$ and $B$. Memory cells $m_0$ and $m_1$ are drained by $B$ in the order they are filled by $A$, and have a capacity of 2 elements. Data constraints are formulated for readability such that the `guard' and `assignment' conjuncts are line-separated.}
	\label{fig:fifo2_rba}
\end{figure}



RBAs have a structure more conducive to \textit{simplification} of the transition space, such that one RBA transition may represent several transitions in a CA. Figure~\ref{fig:fifo2_rba} shows how how this occurs for the \textit{fifo2} connector. Where the CA in Figure~\ref{fig:fifo2_ca} must distinguish the cases where $A$ fills $m_0$ as two separate transitions, the RBA is able to use just one; likewise for the transitions representing cases where $B$ is able to drain $m_1$. This `coalescing' of transitions in RBAs is possible owing to the collapsing of their state space. Even without an intuitive understanding of why such transitions can be collapsed, such cases may often be identified only by inspecting the syntax of the data constraints. For another example of CA, a na{\"i}ve translation to RBA might produce two transitions with data constraints $m=*\wedge{}\;X$ and~$m\neq{}*\wedge{}\;X$ for some~$X$, which are both covered by a single data constraint $X$. As both RBA and RBF share this property, we usually refer to RBA transitions and RBF disjuncts as \textit{rules}, giving these models their name. By distinguishing CA transitions from RBA rules in terminology, we are perhaps more cognizant of the latter's increased ability to \textit{abstract} away needless data constraints. 

\begin{listing}[ht]
	\inputminted[]{java}{putget.java}
	\caption[Type state automaton in Java.]{An example of a program which implements a two-state automaton in the Java programming language. Observe that the behavior of states $A$ and $B$ are encoded implicitly in the \textit{structure} of the program, while determining which of the two in $A$ are available $A$ requires a check ar runtime.}
	\label{listing:putget}
\end{listing}
Typically, Reo has used the $Data$ domains in both CA and RBA as parallels to the data-types of the ports. In most of the languages in which Reo protocols are implemented, the discriminants of such types are not distinguished statically. For example, the C language lacks a way to statically enforce a that function \code{void foo(int x)} is only invoked when $x$ is prime. Instead, checks at runtime are used to specialize behavior. On the other hand, the state-space is simple enough to afford a practical translation into the structure of the program itself, requiring no checking at runtime. For example, Listing~\ref{listing:putget} shows an intuitive representation of a connector that alternates between states $A$ and $B$, getting data $x$ from its environment in $A$, and emitting $x$ when $x=3$. Observe that there is no need to protect operations behind a runtime-check of \textit{which} state the corresponding CA is in. This observation has implications for the behavior of implementations of RBAs, as they `cannot remember' which state they are in and must thus perform more checking. In practice, the overhead of this checking is manageable, and does not \textit{explode} under composition as the state space of CAs tend to do. The representation of automata in programming languages is explored in more detail in Section~\ref{sec:type_state}

\subsection{The Reo Compiler}
TODO ask Sung to summarize the history of the Reo compiler. give a summarized story here. 

The compiler aims to take the low-level implementation of a protocol out of the application developer's hands. Given a protocol specification, the compiler generates the \textit{glue code} and 

TODO focus on RBA

The steps from Reo specification to the generated glue code can be better understood when broken down into stages:

\begin{enumerate}
	\item \textbf{Specification expansion}\\
	The composed definitions of Reo components are unrolled to the channel-level until the protocol is represented by one large automaton with many nodes.
	
	\item \textbf{Minimization}\\
	Nodes not in the protocol's interface are \textit{hidden} and the RBA is minimized. This step produces a new, simpler automaton with the same behavior and interface.
	
	\item \textbf{TODO PUTTERS AND GETTERS}
	
	\item \textbf{Linking and Code Generation}\\
	The finished source code is generated from the resulting internal-representation. Those associated with functions in the target language are linked accordingly, and the rest are parsed and translated from the operational semantics of Reo to suitable target-language operations (such as data movement and duplication). The rules of the internal state are translated to the runtime definition of a protocol component object. An entrypoint to instantiating this protocol object is generated with the appropriate interface. The specifics of this step vary per target language.
\end{enumerate}

\section{Affine Types}

\cite{walker2005substructural}.
look for linear logic. proof theory. look into Rust's motivations. We can't pull things out of the blue. 

talk about TYPE-STATE pattern (aka state machine pattern)
https://hoverbear.org/2016/10/12/rust-state-machine-pattern/
http://cliffle.com/blog/rust-typestate/

\begin{verbatim}
pub struct X([u32;10]);
pub struct Y([u32;10]);


pub fn convert(x: X) -> Y {
Y(x.0)
}

pub fn do_thing_1(x: X) -> u32 {
x.0[0]
}
pub fn do_thing_2(x: X) -> u32 {
let y = convert(x);
y.0[0]
}
-------------------
example::convert:
mov     rax, rdi
mov     rcx, qword ptr [rsi + 32]
mov     qword ptr [rdi + 32], rcx
movups  xmm0, xmmword ptr [rsi]
movups  xmm1, xmmword ptr [rsi + 16]
movups  xmmword ptr [rdi + 16], xmm1
movups  xmmword ptr [rdi], xmm0
ret

example::do_thing_1:
mov     eax, dword ptr [rdi]
ret

example::do_thing_2:
mov     eax, dword ptr [rdi]
ret
\end{verbatim}

Type systems exist for the sole purpose of constraining which programs can be built, and do not add any expressiveness in terms of what can be computed. However, we know that adding (sensible) restrictions provides us other important properties. By constraining ourselves in some part, we drastically increase the number of properties that other parts are safe to assume. Affine types are a type system that applies this reasoning to explicitly managing access to variables, thus, allowing us to statically reason about resource management in a more fine-grained way. 

\subsection{The Rust Programming Language}
\label{sec:rust_language}
introduce Drop, Move, Clone, Copy

Aside from some unusual exceptions\footnote{TODO pinned objects}, all values in rust can be \textit{moved}, which describes a \textit{value} being transferred between variable bindings, or into functions as arguments, as demonstrated in Listing~\ref{listing:move}. Clearly, the Rust compiler tracks which values have been moved. Aside from preserving affine properties, this is necessary for determining when a value should be \textit{dropped}. \code{drop} is the \textit{destructor} function invoked by the compiler on a type when its binding goes out of scope and it has not been moved. 

TODO we use snippets from rust, but omit clutter such as visibility qualifiers, imports and sometimes variable names
TODo enum vs struct.
\begin{listing}[ht]
	\centering
	\inputminted[]{rust}{move.rs}
	\caption[Affine types in the Rust language.]{Type \code{Foo} is affine. On line 7, $x$ is moved into function \code{func}, consuming it. Accessing $x$ is invalid, and so line 8 raises an error.}
	\label{listing:move}
\end{listing}

\subsection{The Type-State Pattern}
\label{sec:type_state}

The \textit{state} or \textit{state machine} pattern refers to the practice of explicitly checking for or distinguishing transitions between and requirements of states in a stateful object\footnote{Usually, we disregard the effects of terminating the program. Equivalently, this pattern only allows one to describe automata in which every `useful' state reaches some final `terminated' state.}. Usually, these states are distinguished in the data domain of one or more types. Even the lowly \code{Option} type can be viewed as a small state machine as soon as some condition statement specializes operations performed with it.

The \textit{type state} pattern is closely related, but as the name suggests, it is characterized by encoding states as types, which usually are distinct from \textit{data} in their significance to a language's compiler or interpreter. A common approach is to instantiate one of the state types at a time. As an example, consider the scenario where a program wants to facilitate alternation between invoking some functions \code{one} and \code{two} which repeatedly mutate some integer $n$. Listing~\ref{listing:typestateeg} gives an example of what this might look like as a \textit{deterministic finite automaton} in the C language. In this rendering, the expression \code{two(one(START)).n} evaluates to the expected result of $(0 + 1) \cdot{} 2 = 2$. Even for this simple example, the encoding of states as types in particular has its benefits; the expression \code{one(two(START))} may appear sensible at first glance, but the compiler is quick to identify the type mismatch on the argument to \code{one}, making clear that the expression does not correspond to a path through the automaton:
\begin{verbatim}
note: expected 'DoTwo' but argument is of type 'DoOne'
\end{verbatim}

The type state pattern can be applied in any typed language, but it is particularly meaningful in languages where the compiler or interpreter \textit{enforces} its intended use. The example above demonstrates some utility, but a language such as C has no fundamental way to prevent the programmer from \textit{re-using} values. 
If the programmer misbehaves, they can retain their previous states when given new ones, and then invoke the transition operations as they please. It's not much of a state machine if all states coexist, is it? This is not always a problem in examples such as the previous. Here, the types prevent the construction of mal-formed \textit{expressions}, and perhaps this is enough. However, we cannot so easily protect a resource from any side effects of \code{one} or \code{two}; imagine the chaos that would result from these functions writing to a persistent file descriptor.
\begin{listing}[ht]
	\inputminted[linenos,tabsize=2,breaklines,frame=lines]{c}{typestate_eg.c}
	\caption[Type state automaton in C with expressions modeling runs.]{An example of the type-state pattern in the C language. The alternating invocation of \code{one} and \code{two} is translated to type-checking the compiler can guarantee. This example guarantees that well-formed \textit{expressions} can be interpreted as valid paths in some corresponding automaton, as the types must match.}
	\label{listing:typestateeg}
\end{listing}

An affine type system overcomes the shortcoming illustrated above. \hl{Formally, \textit{affine types} correspond with \textit{affine} substructural logic, in which the structural rule for `weakening' is absent; essentially, these logics do not consider terms to be idempotent.}
By treating instances of these types as affine \textit{resources}, the programmer cannot retain old states without violating the affinity of the types. The example looks very similar when translated to Rust, but now a case such as that shown in Listing~\ref{listing:typestateeg2} will result in the compiler preventing the retention of the variable of type \code{DoOne}.

\begin{listing}[ht]
	\inputminted[]{rust}{typestate_eg2.rs}
	\caption[Type state automaton in Rust with execution traces as runs.]{A demonstration of how the type-state encoding shown in Listing~\ref{listing:typestateeg} can leverage affine types to ensure that not only expressions, but \textit{a trace through execution} can be interpreted as valid paths through some corresponding automaton. The compiler correctly rejects this example, which corresponds with attempting to take transition \code{two} twice in a row.}
	\label{listing:typestateeg2}
\end{listing}

\subsection{Proof-of-Work Pattern}
\label{sec:proof_of_work}
Section~\ref{sec:type_state} demonstrates how the type-state pattern can be used as a tool to \textit{constrain} actions the compiler will permit the program to do. Indeed, this is a natural parallel to the affinity of the type system, which guarantees that no resource is consumed repeatedly. The counterpart to affine types is \textit{relevant} types, which defines correctness as each resource being consumed \textit{at least once}. Type systems that are both relevant and affine are \textit{linear}, such that all objects are consumed exactly once.

There is no way to create true relevance or linearity in user-space of an arbitrary affine type system; any program which preserves affinity is able to exit at any time without losing affinity. How are we able to enforce a behavior if it is correct to exit at any time? \textit{Proof-of-work} is a special case of the type-state pattern which allows the expression of a relevant type \textit{under the assumption that the program continues its normal flow}; ie. system exits are still permitted. The trick to enforcing the use of some object \code{T} is to specify that a type is a function which must \textit{return} some type \code{R}, and to ensure that \code{R} can \textit{only} be instantiated by consuming \code{T}. Clearly, we cannot prevent \code{T} from being destroyed in some other way, but we are able to prevent \code{R} from being \textit{created} any other way.

Realistic languages have many tools for constraining what users may access. Java has \textit{visibility} to prevent field manipulations. Rust has \textit{orphan rules} to prevent imported traits from being implemented for imported types. Languages without any such features won't be able to prevent users from creating the return type \code{R} without consuming \code{T}. In these cases, another option is \textit{generative types} which, among other things, allow us to further distinguish types with different origins. Here, generative types may be used to ensure not just \textit{any} \code{R} is returned, but a particular \code{R} within our control. As this work uses the Rust language for concrete implementations, we will rely on its ability to prohibit the user from creating \code{R} by using \textit{empty enum types} for types with no data nor type constraints, and by making its fields and constructors \textit{private} otherwise\cite{exotic_sizes}.

Consider the following illustrative scenario: We wish to yield control flow to a user-provided function. Within, the user is allowed to do whatever they wish, but we require them to invoke \code{fulfill}  exactly once (which corresponds to `consuming \code{R}'). How can we express this in terms the compiler will enforce? Listing~\ref{listing:promise} demonstrates a possible implementation (omitting all but the essence of `our' side of the implementation). The user's code would then be permitted to invoke \code{main} with their own choice of callback function pointer. Our means of control is the interplay between dictating both (a)
the \textit{signature} of the callback function and (b) prohibiting the user from constructing or replicating \code{Promise} or \code{Fulfill} objects in their own code.

\begin{listing}[ht]
	\inputminted[]{rust}{promise.rs}
	\caption[Proof of work pattern example of `promises'.]{A demonstration of proof-of-work pattern. Here, the user is able to execute \code{main} with any function as argument, but it must certainly invoke \code{fulfill} exactly once.}
	\label{listing:promise}
\end{listing}

