
\chapter{Background}
\label{sec:background}
\section{Reo}
\label{sec:reo_background}
Reo is a high-level language for specifying protocols. Here, we explore the motivation behind Reo's development, and how the language is used.
The Reo language has applicability whenever there is a benefit in being able to formalize a communication protocol. However, this work primarily focuses on Reo's role in automatic generation of glue-code for applications.  

\subsection{Motivation}
\label{sec:reo_motivation}
\hl{TODO}
%TODO focus on safety properties
%
%Modern software development involves the construction of large and complex projects. Owing to their scale and the heterogeneity of the tasks required, many people are involved in the development of a program at once, and over its development lifetime. The industry has long-since established paradigms for managing the scale of these projects. One tenet of good software design is \textit{modularity}, which describes a structure that, instead of being designed monolithically, is built out of smaller constituent modules. In addition to isolating modules such that they can be re-used in other projects, this design philosophy allows contributors to concentrate on a subset of all the modules at a time. These ideas are well-established in practice; code re-use and separation-of-concerns have been prevalent for some time.
%
%Reo's utility is not only its ability to facilitate modularity. Reo is designed such that properties of the individual modules are \textit{preserved} when modules are \textit{composed} into larger ones. This preservation marks the difference between \textit{gluing} modules together (and hoping for the best), and \textit{composing} them into something guaranteed to have the intended properties. 

\subsection{Language}
\label{sec:reo_lang}
\hl{TODO}
\hl{TODO textual and visual examples eg: alternator}
%Reo is a \textit{coordination} language. This describes its focus on the specification of the \textit{interactions} between distinct actors. This is in contrast to the usual \textit{action-centric} model common to languages with their roots in sequential programming, where the programs or specifications describe \textit{actions} of entities, relegating any associated interactions to requiring derivation from the actions. In a nutshell, Reo provides a language for describing the behavior of a \textit{system} of actors by explicitly constraining the behavior of the \textit{connector} which serves as their communication medium. 
%
%(TODO define connector. same as component just maybe structural?)
%
%The Reo language is essentially graphic; each connector defines a relation over named \textit{locations}. Complex connectors are defined as the composition of simpler connectors over its locations. This inherently visual language is also often seen in its textual form, usually in the context of machine parsing. 
%
%The simplest \textit{primitive} connectors cannot be subdivided
%
%by listing a set of constituent connectors. The simplest primitive Reo connectors are \textit{channels}(TODO channel vs primitive). Channels by definition cannot be subdivided into constituent connectors, as they are defined by either (a) the model that provides Reo's semantics, or (b) opaque components defined in some target language such as C or Java. The nodes themselves are the other important aspect of the language. Ultimately, each node corresponds to a (logical) location which may hold up to one datum at a time. Reo is by default \textit{synchronous}, and relationships between locations propagates that synchrony. \textit{Locations} are divided into two classes according to whether 
%
%This motivates the Reo's metaphor of propagation of data and back-propagation of data-constraints, corresponding to its namesake, the Greek word for `flow'. The compositional aspect is meaningful when locations are involved in multiple relationships.
%
%\textit{forwards} (by moving data several `hops' at once) and backwards
%
%In addition to re-using nodes inside a connector, connectors are able to expose these nodes for re-use in the connectors \textit{above them} by exposing the node in the connector's \textit{interface}. These exposed nodes are called \textit{ports}, leaning on the metaphor of the connector \textit{moving} some data in and out of itself.
%
%(data flow corresponds with what happens at runtime, except its SYNCHRONOUS by default. Relate to TDS. Talk about replication and equality checks).
%
%(In the context of applications, components that cannot be composed compile to things managed by different threads. at the boundaries, they communicate with ports. Here, there is a meaningful difference between putter and getter. Include example of how a protocol that uses some port A three times still results in a putter-getter pair)


%\subsection{Typical Channels}
%
%In principle, Reo does not enforce the use of any primitive channels in particular. Users are free to use channels that are best-suited to their use case. In practice, a small set of exceptionally simple channels are favoured in literature and in practice owing to their versatility. As such, this work presumes that these consititute the majority of the channels out of which our protocols are composed. Below, we enumerate this set of channels and their behavior.
%
%\begin{enumerate}
%	\item sync($I_0, O_0$)
%	\item fifo1($I_0, O_0$)
%	\item lossy\_sync($I_0, O_0$)
%	\item exclusive\_router($I_0, O_0, O_1$)
%\end{enumerate}


\subsection{Semantic Models}
\label{sec:semantic_models}
Reo took a number of years to take its present shape. It is recognizable as early as 2001, but was presented as a concept before it was formalized, leaving it as a task for future work~\cite{jongmans2012overview}. Later, This several different approaches to formal semantics were developed. For our purposes, it suffices to concentrate only on the small subset of the semantics to follow. For additional information, the work of Jongmans in particular serves as a good entry point\cite{jongmans2012overview}.


Starting with the fundamentals, a \textbf{stream} specifies the value of a variable from data domain $D$ changing over the course of a sequence of events. Usually streams are considered infinite, and so it is practical to define them as a function $\mathbb{N}\mapsto{}D$. A \textbf{timed data stream} (TDS) takes this notion a step further, annotating each event in the sequence with an increasing \textit{time stamp}. A TDS is defined by some tuple $(\mathbb{N}\mapsto{}\mathbb{R}, \mathbb{N}\mapsto{}D)$, or equivalently, $\mathbb{N}\mapsto{}(\mathbb{R}, D)$ with the added constraints that time must increase toward infinity~\cite{arbab2004modeling}. By associating one TDS with each \textit{named variable} of a program, one can represent a \textit{trace} of its execution. TDS events with the same time stamp are considered simultaneous, allowing reasoning about \textit{snapshots} of the program's state over its run time. These traces can be practically visualized as \textbf{trace tables}, with variables for columns and time stamps for rows by representing the absence of data observations using a special `silent' symbol \textbf{*}, referring \textit{silent behavior}. In this work, we use `trace tables' to refer to both the visualization and to a program trace as a set of named TDS's. The runs of finite programs can be simulated either by bounding the tables (constraining the TDS domain to be finite), or by simulating finite behavior as infinite by extending the `end' forevermore with silent behavior. Table~\ref{tab:fifo1_eg} gives an example of a trace table for some program with two named variables.


\begin{table}[]
	\centering
	\begin{tabular}{l|cc}
		$\mathbb{R}$  & A & B \\ \hline
		0.0 & 0 & * \\
		0.1 & * & 0 \\
		0.2 & * & * \\
		0.3 & 1 & * \\
		0.4 & * & 1
	\end{tabular}
	\caption[Trace table of a system adherent to fifo1.]{Trace table comprised of TDS's for variables $A$ and $B$. This trace represents behavior that adheres to the \textit{fifo1} protocol with input and output ports $A$ and $B$ respectively.}
	\label{tab:fifo1_eg}
\end{table}


One of it's earlier \textit{coalegebraic models} represented Reo connectors as \textbf{stream constraints} (SC) over such TDS tables in which variables are ports~\cite{arbab2004reo}. Here, constraints are usually defined in first-order \textit{temporal logic}, which allows the discrimination of streams according to their values both now and arbitrarily far into the future\footnote{Not all variants of temporal logic are equally (succinctly) expressive. It requires a notion of `bounded lookahead' to express a notion such as `$P$ holds for the next 3 states' as something like $\square ^{1-3} P$ rather than the verbose $(\square P \wedge \square \square P \wedge \square \square \square P)$.}. This model is well-suited for translating from the kinds of safety properties that are typically desired in practice. Statements such as `$A$ never receives a message before $B$ has communicated with $C$' have clear mappings to temporal logic, as often it is intuitive to reason about safety by reasoning about future events. Table~\ref{tab:fifo1_eg} above shows the trace of a program that adheres the \textit{fifo1} protocol with ports $A$ and $B$ as input and output respectively.

SC are unwieldy in the context of code generation. In reality, it is easier to predicate one's next actions as a function of the \textit{past} rather than the future. Accordingly, \textbf{constraint automata} (CA) was one of the \textit{operational models} for modeling Reo connectors that has a clearer correspondence to stateful computation. Where an NFA accepts finite strings, a CA accepts trace tables. Thus, each CA represents some protocol. Programs are adherent to the protocol if and only if it always generates only accepted trace tables. From an implementation perspective, CA can be thought to enumerate precisely the actions which are allowed at ports given the correct states, and prohibiting everything else by default. A CA is defined with a state set and initial state as usual, but each transition is given \textit{constraints} that prevent their firing unless satisfied; each transition has both (a) the \textit{synchronization constraint}, the set of ports which perform actions, and (b) a \textit{data constraint} predicate over the values of ports in the firing set at the `current' time step. For example, Listing~\ref{tab:fifo1_eg} above is accepted by the CA of the \textit{fifo1} connector with all ports of binary data type $\{0,1\}$. Observe that here the automaton discriminates the previously-buffered value (`remembering' what $A$ stored) by distinguishing the options with states $q_{f0}$ and $q_{f1}$. As a consequence, it is not possible to represent a \textit{fifo1} protocol for an infinite data domain without requiring infinite states.
\begin{figure}[ht]
	\centering
	\begin{tikzpicture}
	\node[state, initial above] (q0) {$q_{e}$};
	\node[state, left of=q0] (q1) {$q_{f0}$};
	\node[state, right of=q0] (q2) {$q_{f1}$};
	\draw
	(q0) edge[bend left, below] node{$\{A\}$\\$d_A$=0} (q1)
	(q1) edge[bend left, above] node{$\{B\}$\\\hspace{1em}$d_B$=0} (q0)
	
	(q0) edge[bend right, below] node{$\{A\}$\\$d_A$=1} (q2)
	(q2) edge[bend right, above] node{$\{B\}$\\\hspace{1em}$d_B$=1} (q0)
	;
	\end{tikzpicture}
	\caption[CA for fifo1 connector.]{CA for the \textit{fifo1} protocol with ports $A$ and $B$ sharing data domain $\{0,1\}$.}
	\label{fig:fifo1_ca}
\end{figure}

Later, CA were extended to include \textit{memory cells} (or \textit{memory variables}) which act as value stores whose contents \textit{persist} into the future. Data constraints are provided the ability to assign to their \textit{next} value, typically using syntax from temporal logic (eg: $m'$ is the value of $m$ at the next time stamp). Figure~\ref{fig:fifo1_ca_mem} revisits the \textit{fifo1} protocol from before. With this extension, the task of persistently storing $A$'s value into the buffer can be relegated to $m$, simplifying the state space significantly. This change also makes it possible to represent connectors for arbitrary data domains, finite or otherwise.



\begin{figure}[ht]
	\centering
	\begin{tikzpicture}
	\node[state, initial] (q0) {$q_{e}$};
	\node[state, right of=q0] (q1) {$q_{f}$};
	\draw
	(q0) edge[bend left, above] node{$\{A\}$\\$d_A=m'$} (q1)
	(q1) edge[bend left, below] node{$\{B\}$\\$d_B=m$} (q0)
	;
	\end{tikzpicture}
	\caption[CA with memory for fifo1 connector.]{CA with memory cell~$m$ for Reo connector~$fifo1$ with arbitrary data domain~$D$ common to ports~$A$ and~$B$. Two states are used to track to enforce alternation between filling and emptying~$m$.}
	\label{fig:fifo1_ca_mem}
\end{figure}


For the purposes of Reo, we are interested in being able to compute the composition of CAs to acquire a model for the compositions of their protocols. Figure~\ref{fig:fifo2_ca} shows an example of such a composition, producing \textit{fifo2} by composing \textit{fifo1} with itself. This new protocol indeed exhibits the desired behavior; the memory cells are able to store up to two elements at a time, and $B$ is guaranteed to consume values in the order that $A$ produced them. Even at this small scale, we see how the composition of such CA have a tendency to result in an \textit{explosion} if state- and transition-space. When seen at larger scales, a \textit{fifo$N$} buffer consists of $2^N$ states. The problem is the inability for a CA to perform any meaningful \textit{abstraction}; here, it manifests as the automaton having to express its transition system in undesired specificity. Intuitively, the contents of $m_0$ are irrelevant when $m_1$ is drained by $B$, but the CA requires two transitions to cover the possible cases in which this action is available. In the context of accepting existing trace tables, data constraints are evaluated predictably. However, in the case of code generation we are able to treat the data constraint instead as a pair of (a) the \textit{guard} which enables the transition as a function of the \textit{present} time stamp, and (b) the \textit{assignment}, which may reason about the next time step, and which we are able to guarantee by \textit{assigning} variables. As such, data constraints are broken up into these parts where possible. Figure~\ref{fig:fifo2_ca} and others to follow formulate their data constraints such that the guard and assignment parts are identifiable wherever it is practical to do so.


\begin{figure}[ht]
	\centering
	\begin{tikzpicture}
	\node[state, initial]      (qee) {$q_{ee}$};
	\node[state, right of=qee] (qfe) {$q_{fe}$};
	\node[state, below of=qfe] (qff) {$q_{ff}$};
	\node[state, below of=qee] (qef) {$q_{ef}$};
	\draw
	(qee) edge[above] node{$\{A\}$\\$m_0'=d_A$} (qfe)
	(qfe) edge[below] node[pos=0.43]{$\quad{}\{\}$\\$\quad{}m_1'=m_0$} (qef)
	(qff) edge[right] node{$\{B\}$\\$d_B=m_1$} (qfe)
	(qef) edge[below] node{$\{A\}$\\$m_0'=d_A$} (qff)
	(qef) edge[left] node{$\{B\}$\\$d_B=m_1$} (qee)
	;
	\end{tikzpicture}
	\caption[CA with memory for fifo2 connector.]{CA with memory cells $m_0$ and $m_1$ for the \textit{fifo2} connector with an arbitrary data domain for ports $A$ and $B$. Transitions are spread over the state space such that the automaton's structure results in the \textit{first-in-first-out} behavior of the memory cells in series.}
	\label{fig:fifo2_ca}
\end{figure}


\begin{figure}[ht]
	\centering
	\begin{tikzpicture}
	\node[state, initial] (q0) {$ $};
	\draw
	(q0) edge[loop above] node{$\{A\}$\\$m=*$\\$\gapwedge{}m'=d_A$} (q0)
	(q0) edge[loop below] node{$\{B\}$\\$m\neq{}*$\\$\gapwedge{}d_B=m\gapwedge{}m'=*$} (q0)
	;
	\end{tikzpicture}
	\caption[RBA for fifo1 connector.]{RBA of the \textit{fifo1} connector for an arbitrary data domain common to ports $A$ and $B$. Memory cell $m$ is used both to buffer $A$'s value, and as part of the data constraint on both transitions for \textit{emptying} and \textit{filling} the cell to ensure these interactions are always interleaved. Data constraints are formulated for readability such that the `guard' and `assignment' conjuncts are line-separated.}
	\label{fig:fifo1_rba}
\end{figure}
Evidently, memory cells provide a new means of enforcing how data persists over time. In many cases, it can be seen that the same connectors can be represented differently by moving this responsibility between state- and data-domains. \textbf{Rule-based automata} (RBA) are the cases of CA for which this idea is taken to an extreme by relying only on memory cells entirely; RBAs have only one state. Figure~\ref{fig:fifo1_rba} models the \textit{fifo1} connector once again, this time as an RBA. Aside from the added expressivity, RBAs benefit from being cheaper to compose. As the state space is degenerate, RBAs may be easily re-interpreted into forms more easy to work with. \textbf{Rule-based form} (RBF) embraces the statelessness of an RBA as a single formula, the \textit{disjunction} of its constraints. In this view, Dokter et al.\ defines their composition of connectors such that, instead of exploding, the composed connector has transitions and memory cells that are the \textit{sum} of its constituent connectors~\cite{dokter2018rule}.



\begin{figure}[ht]
	\centering
	\begin{tikzpicture}
	\node[state, initial] (q0) {$ $};
	\draw
	(q0) edge[loop above] node{$\{A\}$\\$m_0=*$\\$\gapwedge{}m_0'=d_A$} (q0)
	(q0) edge[loop right] node{$\{\}$\\$m_0\neq{}*\gapwedge{}m_1=*$\\$\gapwedge{}m_1'=m_0\gapwedge{}m_0'=*$} (q0)
	(q0) edge[loop below] node{$\{B\}$\\$m_1\neq{}*$\\$\gapwedge{}d_B=m_1\gapwedge{}m_1'=*$} (q0)
	;
	\end{tikzpicture}
	\caption[RBA for fifo2 connector.]{RBA of the \textit{fifo2} connector for an arbitrary data domain common to ports $A$ and $B$. Memory cells $m_0$ and $m_1$ are drained by $B$ in the order they are filled by $A$, and have a capacity of 2 elements. Data constraints are formulated for readability such that the `guard' and `assignment' conjuncts are line-separated.}
	\label{fig:fifo2_rba}
\end{figure}



RBAs have a structure more conducive to \textit{simplification} of the transition space, such that one RBA transition may represent several transitions in a CA. Figure~\ref{fig:fifo2_rba} shows how how this occurs for the \textit{fifo2} connector. Where the CA in Figure~\ref{fig:fifo2_ca} must distinguish the cases where $A$ fills $m_0$ as two separate transitions, the RBA is able to use just one; likewise for the transitions representing cases where $B$ is able to drain $m_1$. This `coalescing' of transitions in RBAs is possible owing to the collapsing of their state space. Even without an intuitive understanding of why such transitions can be collapsed, such cases may often be identified only by inspecting the syntax of the data constraints. For another example of CA, a na{\"i}ve translation to RBA might produce two transitions with data constraints $m=*\wedge{}\;X$ and~$m\neq{}*\wedge{}\;X$ for some~$X$, which are both covered by a single data constraint $X$. As both RBA and RBF share this property, we usually refer to RBA transitions and RBF disjuncts as \textit{rules}, giving these models their name. By distinguishing CA transitions from RBA rules in terminology, we are perhaps more cognizant of the latter's increased ability to \textit{abstract} away needless data constraints. 

\begin{listing}[ht]
	\inputminted[]{java}{putget.java}
	\caption[Type state automaton in Java.]{An example of a program which implements a two-state automaton in the Java programming language. Observe that the behavior of states $A$ and $B$ are encoded implicitly in the \textit{structure} of the program, while determining which of the two in $A$ are available $A$ requires a check ar runtime.}
	\label{listing:putget}
\end{listing}
Typically, Reo has used the $Data$ domains in both CA and RBA as parallels to the data-types of the ports. In most of the languages in which Reo protocols are implemented, the discriminants of such types are not distinguished statically. For example, the C language lacks a way to statically enforce a that function \code{void foo(int x)} is only invoked when $x$ is prime. Instead, checks at runtime are used to specialize behavior. On the other hand, the state-space is simple enough to afford a practical translation into the structure of the program itself, requiring no checking at runtime. For example, Listing~\ref{listing:putget} shows an intuitive representation of a connector that alternates between states $A$ and $B$, getting data $x$ from its environment in $A$, and emitting $x$ when $x=3$. Observe that there is no need to protect operations behind a runtime-check of \textit{which} state the corresponding CA is in. This observation has implications for the behavior of implementations of RBAs, as they `cannot remember' which state they are in and must thus perform more checking. In practice, the overhead of this checking is manageable, and does not \textit{explode} under composition as the state space of CAs tend to do. The representation of automata in programming languages is explored in more detail in Section~\ref{sec:type_state}.

\subsection{The Reo Compiler}
\hl{TODO}
%TODO ask Sung to summarize the history of the Reo compiler. give a summarized story here. 
%
%The compiler aims to take the low-level implementation of a protocol out of the application developer's hands. Given a protocol specification, the compiler generates the \textit{glue code} and 
%
%TODO focus on RBA

\begin{listing}[ht]
	\inputminted[]{text}{fifo1.rba.treo}
	\caption[Fifo1 connector specfication in textual Reo language.]{Textual Reo specification of the \textit{fifo1} connector using RBA semantics. Data is forwarded from input $A$ to output~$B$, buffered in-between in memory cell~$m$.}
	\label{listing:fifo1_treo}
\end{listing}


%The steps from Reo specification to the generated glue code can be better understood when broken down into stages:
%
%\begin{enumerate}
%	\item \textbf{Specification expansion}\\
%	The composed definitions of Reo components are unrolled to the channel-level until the protocol is represented by one large automaton with many nodes.
%	
%	\item \textbf{Minimization}\\
%	Nodes not in the protocol's interface are \textit{hidden} and the RBA is minimized. This step produces a new, simpler automaton with the same behavior and interface.
%	
%	\item \textbf{TODO PUTTERS AND GETTERS}
%	
%	\item \textbf{Linking and Code Generation}\\
%	The finished source code is generated from the resulting internal-representation. Those associated with functions in the target language are linked accordingly, and the rest are parsed and translated from the operational semantics of Reo to suitable target-language operations (such as data movement and duplication). The rules of the internal state are translated to the runtime definition of a protocol component object. An entrypoint to instantiating this protocol object is generated with the appropriate interface. The specifics of this step vary per target language.
%\end{enumerate}

\section{Target Languages}
In this section we introduce terminology relevant to the languages targeted for code generation by the Reo compiler. We identify patterns and properties that are relied upon in later chapters.

\subsection{Affine Type System}
In a nutshell, affine types are characterized by modeling values as finite resources, operations on which \textit{consume} them. This notion of `affinity' has its roots in logic. In a type-theoretic proof system, one can attempt to derive some \textit{judgment} $\Gamma\vdash{}t:\tau$ with \textit{statements} assigning \textit{types} to \textit{terms}; here term~$t$ is stated to have type~$\tau$ under context~$\Gamma$. A context is simply a list of statements, which can be thought to correspond with \textit{assumptions} or \textit{premises}~\cite{nederpelt2014type}. The judgment holds if one can construct a proof, starting with the judgment and select and applying \textit{rules} until no `dangling' judgments remain; the set of available rules characterizes the system. For example, simply-typed lambda calculus has a type-derivation rule for abstraction, substitution and application. Depending on the type system, \textit{structural rules} may additionally be provided for manipulating (`massaging') the context such that other rules may be applied. For example, \textit{weakening} is a common structural rule that allows one to arbitrarily discard statements from the context. Depending on the proof system, this may be useful or even necessary before other rules can be applied such that the proof can be completed. For example, $weakening$ is required to prove $A, t:\tau\vdash t:\tau$, as $var$ cannot be directly applied; $A$ is in the way. One can imagine that the ability to arbitrary replicate, discard and re-arrange terms effectively describes a type system that treats its context like a set.
\[
\begin{aligned}
(var): \frac{}{t:\tau \vdash t: \tau}
&\quad
(weaken): \frac{\Gamma \vdash \Sigma}{\Gamma, A \vdash \Sigma}
&\quad
(contract): \frac{\Gamma, A, A \vdash \Sigma}{\Gamma, A \vdash \Sigma}
\end{aligned}
\]

\textit{Affine} type systems are characterized by the absence of the \textit{contraction} rule. Proofs cannot replicate statements at will, and thus they are a finite resource in the proof, \textit{consumed} by use in rules~\cite{walker2005substructural}.

As type systems do in general, type affinity excludes some programs from being expressed. In the context of programming languages, why would we want this? Of course, the hardware has no problem replicating the bytes representing some integer. Why then do we limit ourselves? This argument can be made for type systems in general; the machine likewise has no problem re-interpreting the bytes storing a string as an integer. This limitation is a feature in and of itself as long as the programs lost are usually somehow `undesirable'. For example, it is exceedingly common practice to dedicate a memory region to one type for the duration of the program. The reasons for this are primarily for the programmer and not the program; it is simply easier to distinguish values by type such that values only move between them when explicitly `moving' in memory.


\subsection{The Rust Programming Language}
\label{sec:rust_language}
The Rust programming language is most similar to C++, being a general-purpose, imperative systems-level programming language with C-like syntax. What sets Rust apart is its memory model. Rust is not a memory-managed language, and has no runtime whatsoever. Instead, the language relies on its \textit{ownership system} to predictably insert allocations and deallocations at the right moment such that it \textit{runs} much as C++ would without exposing these details to the programmer. To make this possible, the Rust compiler keeps track of the variable binding which \textit{owns} a value at all times. Owned values are affine, and associating them with new variable bindings invalidates their previous binding. In Rust, this is called \textit{moving}, and doubles (at least conceptually) with the re-location of a value in memory. Listing~\ref{listing:move} illustrates how this appears to a programmer; In \code{main}, the variable \code{x} is moved into the scope of \code{func}. The subsequent access of \code{x} on line 8 is invalid, preventing this program from compiling. Once an owned value goes out of scope, it is no longer accessible, and Rust performs any destruction associated with that type. Along with the RAII (`resource acquisition is initialization') pattern popularized by C++, programmers can rest assured that their resources are created and destroyed on demand, without the need for any bookkeeping at runtime.

\subsubsection{Borrowing}

On their own, movement is incredibly restrictive; there is no apparent way to use any resource without destroying it. To reclaim some vital functionality, Rust has the \textit{borrow system} to facilitate the creation and management of types whose ownership is \textit{dependent} on others. Similar to those in C++, programmers are able to create \textit{references} to values (also called `borrows' in Rust terms). These references are new types, and thus do not represent a transference of ownership to a new binding. Listing~\ref{listing:move2} demonstrates the example from before, but now passing the $x$ by reference into \code{func} such that \code{x} is not invalidated. The Rust compiler's \textit{borrow checker} relies on variable scoping to keep track of these borrows to ensure they do not out-live their referent, as these would manifest at runtime as \textit{dangling pointers}. This relationship between value and reference is referred to in Rust as the reference's \textit{lifetime}. Rust performs this static analysis at a per-function basis. As such, it is necessary for programmers to fully annotate the input and output types of functions, but they can usually be \textit{elided} within function bodies. This has an important consequence; the compiler does cross the boundaries between functions to interpret their relationship. 

For some types there is no practical reason to enforce affinity. This is usually the case for primitives such as integers. For these cases, the language uses the \code{Copy} trait to opt-out of Rust's affine management of these resources. Copy-types behave in ways familiar to C and C++ programmers. Listing~\ref{listing:move} from before would compile just fine if Rust's 4-byte unsigned integer type~\code{u32} was used in place of~\code{Foo}.

C and C++ have no inherent support for preventing data races. The programmer is in full control of their resources. It is all too easy to create data races to C by unintentionally accessing the same resource in parallel precautions. One of the tenets of Rust's design is to use its ownership system to prohibit these data races at compile-time. For this reason, Rust has an orthogonal system for \textit{mutability}. References come in two kinds: mutable and immutable. The distinction is made explicit in syntax with the \code{mut} keyword. Rust relies on a simple observation of the common ingredient for all data races: mutable aliasing; only \textit{changes} to the aliased (one resource accessible by multiple bindings) resource manifest as data races. Rust's approach is thus simply to prohibit mutable aliasing by preventing these conditions from co-existing. Mutable references must be \textit{unique} (prohibiting aliasing) and immutable references do not allow for any operations that would mutate their referent (prohibiting mutability). This is the same thinking behind the \textit{readers-writer} pattern for the eponymous lock: there is no race condition if readers coexist, but if one writer exists, it must have exclusive access. 




\begin{listing}[ht]
	\centering
	\inputminted[]{rust}{move.rs}
	\caption[Example of move semantics in Rust.]{Type \code{Foo} is affine. On line 7, $x$ is moved into function \code{func}, consuming it. Accessing $x$ is invalid, and so line 8 raises an error.}
	\label{listing:move}
\end{listing}

\begin{listing}[ht]
	\centering
	\inputminted[]{rust}{move2.rs}
	\caption[Example of borrowing in Rust.]{\code{Foo} is an affine resource. New references to \code{x} are created and sent into code{func} without changing the ownership of \code{x}. Rust's \textbf{borrow checker} ensures that these borrows do not outlive \code{x}.}
	\label{listing:move2}
\end{listing}

\subsubsection{Traits and Polymorphism}
The primary means of polymorphism in Rust is through generic types with \textit{trait bounds}, also called `type parameters'. Traits are most similar to interfaces in Java, categorizing a group of instantiable types by defining abstract functions for which implementors provide definitions. Unlike Java, Rust traits say nothing about fields and data, thus describing only their behavior. In this manner, Rust traits are somewhat like C's header files, but rather to be defined per implementor type. Elsewhere in the program, functions and structures can make use of \textit{generic} types. These types are arbitrary and thus opaque to any behavior save for those common to all Rust types; for example, any type at all can be \textit{moved} or can be \textit{borrowed} in Rust. To perform more specialized operations on these generic types, they can be \textit{bounded} with traits. This acts as a contract between the generic context and its concrete call-site; the caller promises that the generic type is reified only with a type which implements the specified traits, and thus the generic can be used in accordance with the behavior these traits provide. This is demonstrated in Listing~\ref{listing:traits}. Here, \code{something} is a function which can be invoked with \code{T} chosen to be any type implementing trait \code{T}, such as \code{String}. 

\begin{listing}[ht]
	\centering
	\inputminted[]{rust}{traits.rs}
	\caption[Generic type example in Rust.]{Definition of a function \code{something} generic over some type \code{T}, where \code{T} implements trait \code{PrintsBool}. Types can implement this trait by providing a definition for all the associated functions, in this case, only \code{print\_bool}.}
	\label{listing:traits}
\end{listing}

Above, we see how a function can be defined such that it operates on a generic type. However, the function cannot be used until the type is chosen concretely for a particular instance. This choice is called \textit{dispatch}, and Rust offers two options: static and dynamic. \textit{Static dispatch} (also called `early binding') is used when the called function can be informed which concrete type has been chosen statically, as the caller knows it at the call-site. During compilation, the generic function is \textit{monomorphized} for the type chosen in this manner on a case-by-case basis, generating binary specialized for that type as if there were no generic at all. Static dispatch is used in C++ templates, and opted-out with the keyword \code{virtual}. This is Rust's second option: \textit{dynamic dispatch} (also called virtual functions or late binding) where the generic function exists as only one instance, and all of the specialized operations on the generic type are resolved to concrete functions at runtime by traversing a layer of indirection: functions are \textit{looked up} in a virtual function table (vtable). Java uses such virtualization extensively, which allows a lot of flexibility such as allowing functions to be \textit{overridden} by downstream inheritants. Precisely how a language represents virtual functions and lays out the data in memory varies from language to language. Rust uses the \textit{fat pointer} representation in its \textit{trait objects}. Concretely, some generic object which is known only to implement trait~\code{Trait} is represented as a pair of pointers; the first pointing to the actual object' data, and the second pointing to a dense structure of meta-data and function pointers for the methods of~\code{Trait}, usually embedded into the text section of the binary by the Rust compiler itself. Both methods of dispatch are exemplified in Listing~\ref{listing:dispatch}, demonstrating how static dispatch must \textit{propagate} generics to the caller for resolution to concrete types at compile time, while one function using dynamic dispatch is able to handle any virtualized types by resolving their methods at runtime.


\begin{listing}[ht]
	\centering
	\inputminted{rust}{dispatch.rs}
	\caption[Static vs dynamic dispatch in Rust.]{Static and dynamic dispatch in Rust exemplified. \code{func\_static} shows the former, propagating the type parameter to the caller. \code{func\_dynamic} shows the latter, relying on a virtual function table to resolve the concrete function at runtime. Function \code{main} shows how both appear at the call site.}
	\label{listing:dispatch}
\end{listing}


Rust uses traits for just about everything. Some traits are defined in the standard library, and have a degree of `first class' status by having special meaning when used in combination with the language's syntax. For example,~\code{Not} is a trait that defines a single function,~\code{not}, which is invoked when the type is negated using the usual exclamation syntax, ie.\ \code{!true}. Some traits have no associated functions, instead exist for the purpose of communicating information to the compiler. Seen before,~\code{Copy} is a trait which disables the Rust compiler's checks of a value's affinity. \code{Copy}-types may be passed by value. On the other hand, \code{Drop} associates the \code{drop} function with a type, which the compiler will invoke when it goes out of scope; this is the parallel to the definition of destructors in C++. It is common practice in Rust to rely upon common traits such as these for frequently-occurring cases. For example, it is considered good practice to implement \code{Debug} on your custom traits such that they can be printed using \textit{debug print} syntax (eg.\ \code{print!("{:?}", foo)}) for programmers depending on your work.

\subsubsection{Enums and Error Handling}
As in C, Rust usually relies on \code{struct} for defining its types. Each is defined as the list of its constituent fields. Creation of these structures necessitates building all of their constituents, and all fields exist at once. By contrast, \textit{sum types} have \textit{variants} only one of which may be present at a time. Arguably, the duck-typing of Python and flexible polymorphism of Java do the work of these sum types; a variable can be bound to \textit{anything} and then its `variant' can be reflected at runtime using some explicit operations (\code{ininstance} and \code{instanceof} respectively). C takes the approach fitting the language's philosophy; \code{union} types represent any one of its constituents but the program is at the mercy of the programmer to interact with it as the correct variant as they see fit. Rust's solution is similar to C's unions, but its focus on safety required the use of \textit{tags}. In Rust, an \code{enum} type is defined with a list of variants, only one of which may exist per intance at a time. At runtime, the variant can be descriminated by explicitly \textit{pattern-matching}, inspecting some implicit meta-data field of the enum stores to reflect which of the variants is in use. Like C's unions, each variant can be an arbitrary type (another struct for example), these variants can be of heterogeneous size, and thus are represeted by the largest of their variants \textit{plus} the space for the tag. 

Unlike Java and Python, Rust has no mechanism for \textit{throwables} which override the default control flow, usually for the purposes of ergonomically handling errors. Instead, Rust represents all recoverable errors in the data domain as enums. The standard library defines \code{Option} and \code{Result} enums, which are monadic in that they \textit{wrap} the `useful' data as one of the variants, but represent the possibility for \textit{other} variants also. They differ in that \code{Option::None} carries no data, and thus \code{Option} is generic only over one type, the contents of the \code{Some} variant. \code{Result}, on the other hand, has two generic types, one for its `successful' \code{Result::Ok} variant, and one for its `unsuccessful' \code{Result::Err} variant. Listing~\ref{listing:enums} gives an example of typical error-handling in Rust; here, \code{divide\_by} relies on \code{Result} to propagate the error for the caller to handle. In circumstances where the error is unrecoverable, Rust uses a thread \textit{panic}, which unrolls the control flow (printing debugging information if an environment variable is set). This is somewhat similar to Java's \code{Error}.

\begin{listing}[ht]
	\centering
	\inputminted[]{rust}{enums.rs}
	\caption[TODO.]{Demonstrating the Rust idiom of using a \code{Result} in return position to propagate exceptions to the caller for handling. Here, \code{main} must \code{match} the return value to acquire the result contained within the \code{Result::Ok} variant.}
	\label{listing:enums}
\end{listing}

\subsection{The Type-State Pattern}
\label{sec:type_state}

The \textit{state} or \textit{state machine} pattern refers to the practice of explicitly checking for or distinguishing transitions between and requirements of states in a stateful object\footnote{Usually, we disregard the effects of terminating the program. Equivalently, this pattern only allows one to describe automata in which every `useful' state reaches some final `terminated' state.}. Usually, these states are distinguished in the data domain of one or more types. Even the lowly \code{Option} type can be viewed as a small state machine as soon as some condition statement specializes operations performed with it. Although its uses are ubiquitous in application development in general, this pattern is particularly useful for those for which the added ability to manage complexity is necessary: video games, for example~\cite{nystrom2014game}.

As the name suggests, the \textit{type state} pattern is an instance of the state pattern, characterized by encoding states as types, which usually are distinct from \textit{data} in their significance to a language's compiler or interpreter. A common approach is to instantiate one of the state types at a time. As an example, consider the scenario where a program wants to facilitate alternation between invoking some functions \code{one} and \code{two} which repeatedly mutate some integer $n$. Listing~\ref{listing:typestateeg} gives an example of what this might look like as a \textit{deterministic finite automaton} in the C language. In this rendering, the expression \code{two(one(START)).n} evaluates to the expected result of $(0 + 1) \cdot{} 2 = 2$. Even for this simple example, the encoding of states as types in particular has its benefits; the expression \code{one(two(START))} may appear sensible at first glance, but the compiler is quick to identify the type mismatch on the argument to \code{one}, making clear that the expression does not correspond to a path through the automaton:
\begin{verbatim}
note: expected 'DoTwo' but argument is of type 'DoOne'
\end{verbatim}

The type state pattern can be applied in any typed language, but it is particularly meaningful in languages where the compiler or interpreter \textit{enforces} its intended use. The example above demonstrates some utility, but a language such as C has no fundamental way to prevent the programmer from \textit{re-using} values. 
If the programmer misbehaves, they can retain their previous states when given new ones, and then invoke the transition operations as they please. It's not much of a state machine if all states coexist, is it? This is not always a problem in examples such as the previous. Here, the types prevent the construction of mal-formed \textit{expressions}, and perhaps this is enough. However, we cannot so easily protect a resource from any side effects of \code{one} or \code{two}; imagine the chaos that would result from these functions writing to a persistent file descriptor.
\begin{listing}[ht]
	\inputminted[linenos,tabsize=2,breaklines,frame=lines]{c}{typestate_eg.c}
	\caption[Type state automaton in C with expressions modeling runs.]{An example of the type-state pattern in the C language. The alternating invocation of \code{one} and \code{two} is translated to type-checking the compiler can guarantee. This example guarantees that well-formed \textit{expressions} can be interpreted as valid paths in some corresponding automaton, as the types must match.}
	\label{listing:typestateeg}
\end{listing}

An affine type system overcomes the shortcoming illustrated above. By treating instances of these types as affine \textit{resources}, the programmer cannot retain old states without violating the affinity of the types. The example looks very similar when translated to Rust, but now a case such as that shown in Listing~\ref{listing:typestateeg2} will result in the compiler preventing the retention of the variable of type \code{DoOne}.

\begin{listing}[ht]
	\inputminted[]{rust}{typestate_eg2.rs}
	\caption[Type state automaton in Rust with execution traces as runs.]{A demonstration of how the type-state encoding shown in Listing~\ref{listing:typestateeg} can leverage affine types to ensure that not only expressions, but \textit{a trace through execution} can be interpreted as valid paths through some corresponding automaton. The compiler correctly rejects this example, which corresponds with attempting to take transition \code{two} twice in a row.}
	\label{listing:typestateeg2}
\end{listing}

\subsection{Proof-of-Work Pattern}
\label{sec:proof_of_work}
Section~\ref{sec:type_state} demonstrates how the type-state pattern can be used as a tool to \textit{constrain} actions the compiler will permit the program to do. Indeed, this is a natural parallel to the affinity of the type system, which guarantees that no resource is consumed repeatedly. The counterpart to affine types is \textit{relevant} types, which defines correctness as each resource being consumed \textit{at least once}. Type systems that are both relevant and affine are \textit{linear}, such that all objects are consumed exactly once.

There is no way to create true relevance or linearity in user-space of an arbitrary affine type system; any program which preserves affinity is able to exit at any time without losing affinity. How are we able to enforce a behavior if it is correct to exit at any time? \textit{Proof-of-work} is a special case of the type-state pattern which allows the expression of a relevant type \textit{under the assumption that the program continues its normal flow}; ie. system exits are still permitted. The trick to enforcing the use of some object \code{T} is to specify that a type is a function which must \textit{return} some type \code{R}, and to ensure that \code{R} can \textit{only} be instantiated by consuming \code{T}. Clearly, we cannot prevent \code{T} from being destroyed in some other way, but we are able to prevent \code{R} from being \textit{created} any other way.

Realistic languages have many tools for constraining what users may access. Java has \textit{visibility} to prevent field manipulations. Rust has \textit{orphan rules} to prevent imported traits from being implemented for imported types. Languages without any such features won't be able to prevent users from creating the return type \code{R} without consuming \code{T}. In these cases, another option is \textit{generative types} which, among other things, allow us to further distinguish types with different origins. Here, generative types may be used to ensure not just \textit{any} \code{R} is returned, but a particular \code{R} within our control. As this work uses the Rust language for concrete implementations, we will rely on its ability to prohibit the user from creating \code{R} by using \textit{empty enum types} for types with no data nor type constraints, and by making its fields and constructors \textit{private} otherwise\cite{exotic_sizes}.

Consider the following illustrative scenario: We wish to yield control flow to a user-provided function. Within, the user is allowed to do whatever they wish, but we require them to invoke \code{fulfill}  exactly once (which corresponds to `consuming \code{R}'). How can we express this in terms the compiler will enforce? Listing~\ref{listing:promise} demonstrates a possible implementation (omitting all but the essence of `our' side of the implementation). The user's code would then be permitted to invoke \code{main} with their own choice of callback function pointer. Our means of control is the interplay between dictating both (a)
the \textit{signature} of the callback function and (b) prohibiting the user from constructing or replicating \code{Promise} or \code{Fulfill} objects in their own code.

\begin{listing}[ht]
	\inputminted[]{rust}{promise.rs}
	\caption[Proof of work pattern example of `promises'.]{A demonstration of proof-of-work pattern. Here, the user is able to execute \code{main} with any function as argument, but it must certainly invoke \code{fulfill} exactly once.}
	\label{listing:promise}
\end{listing}
