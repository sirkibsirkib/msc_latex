\chapter{Introduction}
Traditional, sequential programming has been changing for decades. Over time, languages acquired more and more tools to manage the level of abstraction, such that programs were of higher quality, and with a lower cost to develop~\cite{shaw1984abstraction}. This trend continues to this day. For example, conventionally imperative languages such as Java and C++ have since added functional features, such as closures, to capitalize on their brevity and lack of side-effects. 

By comparison, concurrent programming has fallen behind. Although some strides have been made, it is not uncommon for modern programs to dip down to the level of channels, mutex locks and semaphores, which have remained unchanged for decades. As a consequence, large concurrent programs require arcane knowledge, inhibiting the employability of programmers, and retaining the brittleness and obfuscation common to the programs of their day~\cite{chamberlain2007parallel}. This deficiency has not gone unnoticed, and many academic and industrial projects have sought to fill the void. Paradigms such as the \textit{actor model}, various \textit{process calculi} and more have emerged, each offering their take on the right approach to managing abstraction. Farhad et al.\ observed that despite their innovation, such approaches inherited a vestige of their connection to the world of sequential programming: \textit{action-centricism}, ie.\ making explicit the individual actions contributing to interactions, rather than the interaction itself. Although many modern approaches introduce valuable abstractions, they often have in common that they still relegate interactions to a derived concept~\cite{arbab2011puff}. In such programs, the burden is on the programmer to conceptualize the program's runtime behavior to reconstruct these interactions to understand the `big picture'. As programs grow large, these actions become far-flung and interact in complex and unexpected ways. Needless to say, such programs are difficult to reason about, and costly and error-prone in their maintenance.

Reo is an exogenous coordination language developed at the CWI in Amsterdam. The language is designed around the representation of \textit{protocols} which specify the communication between actors as a first-class concept. Reo protocol specifications are dense, self-contained, and declarative, so that it can be easily understood and manipulated by humans and machines alike. Protocols are understood as \textit{relations} between actors, specifying how data is allowed to `flow' between abstract \textit{ports}, each of which send and receive values through the system, oblivious of the protocol and which other ports exist. The idea is to separate (often sequential) \textit{computation work} into weakly-coupled modules, with Reo defining their \textit{communication} only~\cite{arbab2004reo}. With this approach, protocols and compute-components become swappable, re-usable modules; each can be more easily understood, maintained and re-used. The Reo language is designed such that the protocols behave as expected when \textit{composed}; one can reason about components in isolation, safe in the knowledge that their properties are preserved in the system at large. An ecosystem of tooling has sprung up around the Reo language to make use of these explicit protocol specifications, ranging from visualization to verification. 

In this work, we focus on Reo's use in \textit{code generation}. Instead of writing and maintaining an action-centric codebase, programmers are able to write computation code in their language of choice, with \textit{components} independently sending and receiving data through opaque port objects, whose communication is defined separately as a Reo protocol. The \textit{Reo compiler} generates the action-centric glue code as if they programmer had written it themselves, with the guarantee that it behaves as specified~\cite{jongmans2013modularizing}. In this manner, programmers are able to understand and manipulate their code given the best of both worlds: a high-level view into abstract and modular components, while compiled and executed using the language of their choice. Over time, the Reo compiler has been extended to support various compilation-target languages, with Java seeing the most extensive development. 

In this work, we extend the Reo compiler's repertoire of supported language targets by designing and implementing a Rust back-end. Chapter~\ref{sec:imperative_form} describes how this translation is performed. Rust is a favourable choice, as it is easily inter-operable with C and C++, and has a strict type system that allows us to expose an API that guarantees conformance with Reo's intuitive value-passing semantics. Chapter~\ref{sec:protocol_runtime} explains how we are able to leverage Reo's explicit protocols in combination with Rust's systems-level resource management to safely implement various optimizations, such as transparent \textit{reference-passing} for components coordinating in shared-memory. Chapter~\ref{sec:api} investigates leveraging Rust's \textit{affine type system} to offer an ergonomic mechanism to inject \textit{liveness properties} of our Reo-coordinated Rust programs by checking our compute-code for \textit{protocol adherence} at compile time without significant overhead at runtime.

