\chapter{Protocol Translation}
\label{sec:imperative_form}
In this chapter, we describe the process of translating the Reo compiler's internal representation of a protocol specification into an executable \textit{protocol object} in the Rust language.

\section{Two-Phase Code Generation}
In this section, we explain and motivate our approach of segmenting the code generation process into two distinct phases. Throughout, we refer to the precedent set by the existing Reo compiler backend for generating code in the Java language, as it has seen the language most similar to Rust which has seen significant development.   

\subsection{Generation Sub-tasks}
\label{sec:sub_tasks}
Reo specifications represent connectors declaratively as relations between ports. They are thus well-suited to reasoning about the protocol's properties. In contrast, our target imperative languages such as Java and Rust represent computation such that it corresponds more closely to machine instructions; they are imperative, laying out sequences of actions which together emerge as interaction at runtime. Where interactions in the former can be oriented around the synchronous observations of port values, interactions of the latter must be expressed as sequences of actions, laid out over time. Implementing algorithms for translating between these forms must take care that the translation procedure between these forms preserves the semantics as intended; choosing the incorrect ordering can change the nature of the emergent interaction in unexpected ways. For example, reading memory cell \textit{before} writing it corresponds to a different interaction than reading it \textit{after} writing it. 

Java, Rust and Reo have in common that they are strongly-typed languages. Reo's specifications are permitted a degree of \textit{type elision}; for the sake of programmer ergonomics, the data-types of ports may be omitted, such that they can later be derived in context. Rust shares this property, and so the Rust compiler works to \textit{resolve} data types during compilation. Circumstantially, these elisions may produce cases for which a correct resolution is impossible, as the type annotations or constraints present are in conflict. Our task is to emulate this work ourselves, assigning concrete types for port objects in our emitted code such that is guaranteed to type-check in the Rust language. Failure to do this correctly would result in Reo emitting code rejected by the Rust compiler. This would not be a threat to correctness, but it results in significant inconvenience to the programmer.

Regardless of the intermediate representation, protocol objects must ultimately be emitted in the target language. Aside from expression in the correct syntax, the end result must make explicit any work required to make it \textit{executable} with the desired runtime behavior. Even simple concepts require the support of auxiliary book-keeping structures to maintain the protocol object's state, and specialized \textit{concurrency primitives} are needed to ensure that actions compose into interactions at runtime in the expected way. Clearly, this is very particular to the target language, as they vary greatly on how they fundamentally express operations on data at a granular level.

In summary, we identify and name three sub-tasks of generating target language protcol objects from a Reo protocol description:
\begin{itemize}
	\item [$T_{seq}$] Declarative interactions must be decomposed into sequences of imperative actions.
	\item [$T_{type}$] Ports must be given data types such that they agree with any type-annotations in the Reo specification, and successfully type-check in the target language.
	\item [$T_{run}$] Details necessary to make the result runnable are included. Symbolic actions are represented as concrete operations on data.
\end{itemize}


\subsection{Decoupling the Reo Compiler from Rust}
\label{sec:decoupling_reo_rust}
The Reo compiler has an existing backend for generating Java code. It works by generating Java according to the structure of a \textit{template generator}. In this manner, it can be thought of as performing all code-generation sub-tasks at the same time directly from the compiler's intermediate representation. However, the extent to which the Reo compiler is \textit{coupled} to the Java language is reduced through the reliance on a Java library for the granular implementations of structures that are common to all protocol objects; rather than generating these classes each time, the Reo compiler simply generates a dependency. For example, the library defines a \code{Component} interface, for which the code generator produces a protocol-specific implementor class. Consequently, a significant part of the $T_{run}$ sub-task (sub-tasks are defined in Section~\ref{sec:sub_tasks}) is delegated to this library.

For $T_{type}$, help comes from the Reo compiler itself, which in its current form was developed with support for Java in mind. This is visible in its internal representation. For example, types for which no explicit data type annotation was included are assigned the \code{Object} type, which encapsulates all types that may be concretely chosen for transmission through ports. This design essentially uses \code{Object} as an all-encompassing \textit{sum type}, relegating the task of \textit{type reflection} (determining concretely which `variant' of \code{Object}) to the user themselves. This approach is sufficient in the case of Java, as \code{Object} supports all the behavior relied about by the Reo protocol object at runtime, namely (1) data-equality checks, (2) data movement, and (3) data replication\footnote{In the chapter to follow we discuss how this approach introduces safety concerns. In a nutshell, Reo-generated Java assumes that the replication of \code{Object} references preserves Reo's value-passing semantics. This is not the case, as it may result in \textit{data races}.}.

Only $T_{seq}$ is performed almost entirely by the template generator. For simple protocols, this task is relatively easy, as there is not much to add when actions are largely concurrent. For example, replicating the contents of a memory cell into a set of others is simply-done in Java by first reading an object reference, and then overwriting the others one at a time. However, ordering dependencies must be resolved very carefully in the general case. The current Java code generator is susceptible to erroneously observing value $x$ at memory cell $M$ in the event that the observation is synchronous with $M$'s value being overwritten by $x$. Even with the help of the template generator, this translation is sufficiently complex to make detection of these bugs difficult.

Rust is able to mimic Java's approach to create a similar backend through the explicit use of \textit{dynamic dispatch}, such that types can be collapsed to something analogous to Java's \code{Object} class. If done na\"ively, the resulting backend would inherit the problems of its predecessor, and new ones to boot; the Java-like approach is not idiomatic in the context of Rust; it would not make good use of the extensive control of systems resources unique to a systems language. Chapter~\ref{sec:protocol_runtime} to follow goes into detail about the properties of the protocol runtime. Here, it suffices to say that we wish to implement a runtime that does not rely on heap-allocation of its port-values, and thus cannot rely entirely on dynamic dispatch. Furthermore, our runtime wishes to perform more extensive optimizations, relying on the unique abilities of our systems language to manipulate its resources at a low level. All these extensions pose a problem in particular for $T_{run}$, as runtime properties directly influence how the executable protocol objects are represented. Our work in unremarkable in its solution to this problem: we delegate~$T_{seq}$ to a Rust library. However, we make this separation more extreme. In a nutshell, we wish to partition the work of code generation into two clear \textit{phases}, the former of which performs tasks~$T_{seq}$ and~$T_{type}$, and the latter of which performs $T_{seq}$. To minimize coupling between the modules performing these tasks, the interface between phases is made terse, unambiguous and explicit in the definition of a new intermediate representation of protocol connectors: the \textit{imperative form}.

\subsection{Temporary Simplifications}
Our intention is to isolate the Reo backend from Rust's specifics as extensively as possible. In this manner, we decouple the modules responsible for the code-generation subtasks in accordance with good software engineering practices. Furthermore, it facilitates the \textit{re-use} of the first phase of the code generation process for \textit{other} imperative programming language targets. The section to follow defines imperative form to be as target-language agnostic as possible. However, for the sake of minimizing the disturbance to the Reo tooling ecosystem, we still embrace the per-target structure for Reo code generation for now. As such, the Reo compiler still specifies a Rust language target, and emits executable Rust source as a result. Our representation of the \textit{imperative form} is expressed in Rust syntax (as the \code{ProtoDef} type) such that this reliance on an intermediate representation is invisible to the end user. As far as they are concerned, Reo generates native Rust that just happens to \textit{somehow} make minimal use of Rust-specific syntax. \code{ProtoDef} corresponds closely to the definition of imperative form, facilitating this decision being overturned in future with minimal effort.


\section{Imperative Form}
\label{sec:imperative_form_sec}
In this section, we define our new intermediate representation of Reo protocol specifications. We include an intuitive look at how it captures the details of the Reo compiler's internal representation, but such that only $T_{seq}$ remains to be performed before the finished Rust source code can be emitted.


\subsection{Concept}
Imperative form represents a protocol whose translation from Reo to an imperative language has been completed as fully as possible, but stopped short of introducing implementation- and language-specifics. Thus it is still a specification, free from particular syntax, and with types and names that are \textit{symbolic}, needing to be resolved in the manner befitting the target language. In terms of the generation sub-tasks defined in Section~\ref{sec:sub_tasks}, imperative form represents the completion of~$T_{seq}$ and~$T_{type}$, but not~$T_{run}$.

The translation from Reo's internal representation to imperative form is \textit{lossless}, and so any language compiling from the former would also be able to compile from the latter. However, the utility of this representation is the increase in \textit{explicitness}, capturing the result of significant preprocessing toward some imperative target. As the fine details of these languages differ, imperative form is not a perfect fit for them all to the same extent. Languages are best-served if their \textbf{assumptions} align with those made by the imperative form itself, both of which are introduced during~$T_{seq}$\footnote{Imperative form assumes that the target language can assign static data types to ports. However, this assumption is shared by Reo itself, and does not present a problem for untyped languages. For imperative languages without types, ports can simply share some~\code{Any} type, satisfying this assumption trivially.}:
\begin{enumerate}
\item The value of a variable cannot be read before it is computed.
\item Actions can be ordered into a sequence of two sub-sequences $S_a$ and $S_b$, such that $S_a$ does not result in \textit{observable effects} beyond the protocol.
\end{enumerate}

\subsubsection{Rules as Transactions}

The Reo compiler's internal representation partitions the work of a rule into its \textit{guard} and \textit{assignments}. This is already a step in the direction of imperative computation, observing that some work (the guard) must be performed \textit{prior} to deciding whether the rule \textit{fires}, in which case the assignment follows. As the protocol does not define the moments when it will evaluate the guard, it is necessary that this evaluation has no side effects ie.\ observable effects to the outside world. Assumption 2 generalizes this notion, representing the sequence of an interaction's constituent rules broken into subsequences, with $S_a$ and $S_b$ corresponding to guard and assignment respectively.

We elaborate our intuition by noting that mutations of the protocol's state do not result in observable effects as long as the effect is \textit{reversed} before becoming observable. We are therefore able to support operations in $S_a$ that manipulate persistent variables so long as there is a well-defined means of being reversed. Ultimately, we are able model imperative form rules as \textit{transactions}, where the instant of \textit{commitment} separates $S_a$ from $S_b$. Reversible actions occur in $S_a$, and persistent actions occur in $S_b$. At any time prior to commitment, a special \textit{meta-action} in $S_a$ can initiate a \textit{rollback}, reversing the effects of transient actions and aborting.

\subsubsection{Action Granularity}

Imperative form represents a protocol's defined interaction as actions to be computed in the specified sequence. At this stage, our representation is still symbolic; actions do not necessarily correspond 1-to-1 with those in the target imperative language\footnote{Clearly this is impossible, as different languages express the same work at a different granularity of actions.}. How the these symbolic rules are representing in the final language is not specified. Consequently, imperative form walks the fine line between between preprocessing and under-specification with the choice of the granularity of its actions; the finer the granularity, the more (over-)specified the action sequence. 

From assumption 2, imperative form can be understood to represent any rule with \textit{at least} two actions, in which case $S_a$ and $S_b$ are singleton sequences such that the rule's assignment ($S_b$) follows the evaluation of the rule's guard ($S_a$). Further elaborations of ordering are introduced as a consequence of assumption 1; actions are fragmented as much as necessary to ensure that the creation of any \textit{temporary variables} occurs before its value is ever \textit{accessed}. For simple protocols involving no temporary variables, assumption 1 does not come into play and the resulting imperative form corresponds closely to the Reo compiler's internal representation.

Complex connectors require more ordered actions to represent the computation and evaluation of temporary variables. For example, consider a protocol in RBF with data constraint $f(P_0)=f(P_1)$ and synchronization constraint $\{P_0, P_1\}$ with input (putter) ports $P_{0-1}$. This rule can be understood as ``$P_{0-1}$ fire \textit{if} the results of function~$f$ on their put-values are equivalent''. Here, the results of $f$ clearly cannot be compared until \textit{after} they are computed. The imperative form representation includes specifies that two temporary variables be computed and stored for terms $f(P_0)$ and $f(P_1)$. As the rule's firing is determined as a function of these variables, their creation \textit{must} be present in $S_a$ in particular. This allows the program to rely on being able to abandon the rule unfired during $S_a$ without fear of observable effects.

A final elaboration on the sequence of actions is necessary to account for memory cells whose values~$x$ are synchronously read and overwritten. Reo's internal representation inherits syntax from \textit{temporal logic} to disambiguate reads before and after overwrites for such cases: $m$ and $m'$ represent the current and `next' values respectively, with overwriting occurring in-between. The reader is likely familiar with the common solution to such problems in an imperative context; some temporary variable stores the old value to avoid it being overwritten. This is represented here by including an action in $S_a$ which swaps the contents of two memory cells. This corresponds neatly with the canonical imperative solution, and can be easily \textit{reversed} such that they can occur in $S_a$.

\subsubsection{Data Movement}
The Reo compiler's internal representation represents all a rule's \textit{observable effects} as a set of assignments. Equivalently, assignments are a partial mapping from \textit{destination} (a subset of the protocol's \textit{getter} ports, and empty memory cells) to their assigned value. Imperative form has in common that it represents all observable effects as these assignments in this concurrent fashion. Concretely, $S_b$ is always defined as a single, coarse-grained action specifying the destination of values in the result of the rule firing. However, it represents this mapping turned on its head; expressed as a mapping from \textit{source} (a set of values) to sets of destinations. This representation is chosen for its orientation making more easy to track the movement of each datum. The intuition is that this representation makes it easier to treat values as \textit{resources} that potentially require delicate management. Depending on the nature of the target language, it may be necessary to check the \textit{affinity} or \textit{relevance} (see Section~\ref{sec:affine_type_systems} for definitions) of these values, as it becomes of paramount importance to control the number of destinations per value. As the imperative form is language-agnostic, it makes no attempt to rectify this problem itself, relying on the $T_{run}$ task to resolve the problems in the manner befitting the target language after the fact.

As an example, the Rust language is concerned with the \textit{affinity} of some its data types, and is able to handle data-movements with numerous destinations per datum by careful injection of \code{clone} calls; creating new affine resources such that the original is replicated safely. This is discussed further in Section~\ref{sec:translation_phase_2} to follow.

\subsection{Definition}
\label{sec:imperative_form_definition}
A protocol description in imperative form must be provided by a single structure which provides mappings for symbolic names such that they can be resolved as they are encountered when traversing the rest of the definition. This \textit{name definition} structure is similar to a \textit{symbol table}. The behavior of the connector itself is defined by a set\footnote{In our implementation, these rules are provided in an ordered list, primarily for the purpose of making for more comprehensible error messages.} of \textit{imperative rules}, each corresponding to an RBA rule, given by a tuple $(P, I, M)$ with:
\begin{enumerate}
	\item \textbf{Premise $P$}\\
	A tuple of three \textit{identifier} sets $(P_R, P_F, P_E)$. $P_R$~contains the set of identifiers whose values must be `ready', and are thus in stable state, involved in the rule. The subset of identifiers belonging to ports thus encodes the \textit{synchronization constraint} of the associated RBA. $P_F$ and $P_E$ are the sets of \textit{memory variables} which must be known to be full and empty respectively, such that it is known whether they can be read or written from. The rule can certainly not consider firing unless all ports are ready and all memory cells are in the specified states.
	
	\item \textbf{Instructions $I$}\\
	A list of reversible \textit{instructions} which are performed in sequence. These instructions have no immediately observable effects, such that they can be reverted in the event of a \textit{rollback}. Concretely, each instruction is one of:
	\begin{itemize}
		\item $check(p)$\\
		Trigger a rollback if predicate $p$ over data is satisfied.
		\item $fill_P(m, p)$\\Fill an empty memory variable $m$ with the result of a predicate $p$ over available data. It's data type is implicitly \textit{boolean}.
		\item $fill_F(m, f, a)$\\
		Fill an empty memory variable $m$ with the result of invoking function $f$ with parameters $a$, a list of references to data variables with length matching the arity of $f$. It is incorrect for $f$ to \textit{mutate} its arguments, as this would result in observable effects which cannot be rolled back.
		\item $swap(m_0,m_1)$\\
		Swap\footnote{In principle, any reversible data-agnostic manipulation is possible, but swapping values is sufficiently expressive and intuitive for our purposes.} the values in two memory variables~$m_0$ and~$m_1$.
	\end{itemize}
	If a rollback is triggered by $check$, any swapped memory cells are swapped back, and any memory cells whose values were created by $fill_P$ or $fill_F$ are destroyed.
	
	\item \textbf{Movements $M$}\\
	A mapping from \textit{resources} to any identifiers that can act as getters (getter ports and empty memory cells). This represents the \textit{observable effects} of the rule firing after instructions are performed without triggering rollback. 
\end{enumerate}

As an example to demonstrate this representation, the RBA rule in the previous section with data constraint $f(P_0)=f(P_1)$
and synchronization constraint $\{P_0, P_1\}$ can be represented in the imperative-form rule with:

\noindent{}
\begin{tabular}{r|l}
	rule element	&  value \\ \hline
	premise	&  $(\{P_0, P_1\}, \{\}, \{t_0,t_1\})$ \\
	instructions	& $[fill_F(t_0, f, [P_0]), \quad{} fill_F(t_1, f, [P_1]), \quad check(t_0 = t_1)]$ \\
	movements	& $\{\}$ 
\end{tabular}
\vspace{1em}

For our purposes, omitting a source identifier $s$ from~$M$ can be interpreted as $s\rightarrow{}\{\}$ by default, understood to mean `discard $s$'. For other applications, it may be preferable to require that each source have an explicitly-defined destination set. For example, this would be useful in the context of a \textit{linear type system} in which data cannot trivially be \textit{destroyed}. 



\section{Translation Pipeline}
In this section we explore the translation procedure of protocols, starting from their declarative Reo specification in the Reo compiler's internal representation to executable Rust. This is done in two distinct phases, as explained in Section~\ref{sec:decoupling_reo_rust}. 

\subsection{Phase 1: Reo Compiler Backend}
\label{sec:translation_phase_1}
The Rust backend of the Reo compiler translates from the compiler's intermediate representation to Rust. Section~\ref{sec:decoupling_reo_rust} explains how this task is partitioned such that the compiler itself only performs the first of the two phases that comprise this process. Rust is still the target, but rather than generating the protocol object to be executed directly, Reo outputs a source file that contains a \code{ProtoDef}, the Rust-syntax version of a protocol in \textit{imperative form}. 


\subsubsection{Action Sequencing}
The most obvious role the backend plays in the translation process is to spread the contents of the relational, interaction-oriented internal representation over the imperative, action-oriented \textit{imperative rules}. This task is called $T_{seq}$ in the previous sections. The process begins given (1) the set of port putters in the synchronization set, (2) a mapping from port-getter to \code{Term}, (3) a mapping from port-putter to \code{Term}, and (4) a \code{Formula}, representing the rule's guard. Terms and formulas are recursive data structures, and can both be thought of as \textit{sum types}. The former describes a an expression to be computed at runtime. For example, \code{MemoryCell(A)} and~\code{True} are both terms that can be assigned to memory cells and retrieved by getter-ports. \code{Formula} is similar, but used in the context of predicates; formulas are essentially terms known to have a boolean data-type, and thus come with variants for the usual boolean operations. For example, \code{True}, \code{And(Eq(A,B), Eq(B,C))} are examples of formulas. The backend traverses the compiler's internal representation, constructing imperative form rules one at a time, also populating the associated \code{NameDef} symbol table. For each such rule, it helps to consider each of its distinct structures in isolation, initially without any optimizations:

\begin{enumerate}
	\item [$P$] \textbf{Premise}\\
	
	The first task of the backend is to build the imperative rule's \textit{premise}. Observe that the this step is nearly trivial, as the synchronization set, set of port-getters, and set of port-putters are already provided. The only exception is for memory cells which are \textit{read}, which must be derived by encountering their occurrences inside terms and formulas for the guard and assignments. Clearly, no information is added by explicitly isolating the set of readable memory cells; we are able to derive it despite being absent in the internal representation, and the term is only meaningful if one cannot read from a memory cell \textit{unless} it is marked as readable. We nevertheless perform this preprocessing as it makes explicit that the memory cell must be available for the interaction without having to `look ahead' into terms and formulas. Furthermore, its inclusion provides us a simplifying property: if an identifier does not occur in the premise, it must refer to a \textit{temporary variable}.
	
	\item [$I$] \textbf{Instructions}\\
	Next, the backend generates a list of \textit{instructions}, representing the \textit{tentative actions} that may be rolled back if the rule aborts instead of committing. Here, $fill_P$ and $fill_F$ instructions are inserted to compute the results of any evaluations of predicates or functions and bind them to temporary variables. Next, special cases of each memory cell~$M$ synchronously read-from and written-to are detected. For each, a temporary variable $T_M$ is created, and an instruction $swap(M, T_M)$ is appended to~$I$. As a result, data movements can be conducted in parallel without fear of a race condition emerging; although $M$ is logically involved both in reading and writing, these actions are distinguished by name; $M$ is as independent from $T_M$ as any two distinct memory cells. $M$ receives and stores its new (written) value $M$, and its old value can concurrently be read from $T_M$ without interference. Finally, a \textit{check} instruction is appended to the list, parameterized with an analog to the compiler's \code{Formula} type. 
	
	\item [$M$] \textbf{Movements}\\
	All port-getters and memory cells which are \textit{written to} are grouped by source to produce~$M$, a mapping from sources to destination sets. Each mapping is additionally annotated with a flag to indicate whether the value is retained at the source, ie.\ the mapping is of type $Identifier \Mapsto{} (boolean, \{Identifier\})$. As an example, moving a value from~$A$ to~$B$ results in $A$ mapping to $(\text{\textit{false}}, \{B\})$.
	
\end{enumerate}
Clearly, many opportunities exist for optimization of the imperative form, as it affords some degree of \textit{over}-specification and can represent the same connector with a different sequence of instructions. As an example, it is possible to fragment our guard into several \textit{check} instructions. This can be used to achieve \textit{short-circuiting} behavior; as long as no guard reasons about a temporary variable \textit{prior} to its definition, a check may be moved earlier such that rollbacks may be triggered earlier and work can be avoided. These opportunities were not explored extensively in this work, as instructions in practice tend to be simple enough not to afford many such opportunities.
More significant is the \textit{omission} of trivial checks entirely. This can be done by traversing instructions with a model of the protocol's state, initialized to match the premise. Checks known to always hold can be omitted, and checks known to never hold can result in the rule being discarded entirely (it would never fire!). This optimization is incorporated, and works to great effect in practice.


\subsubsection{Type Classification and Constraining}
Previously referred to as $T_{type}$, the backend must extract data-type information about its ports and memory cells from the compiler's internal representation such that the generated Rust is valid. Our approach is to begin by assuming that every port, memory cell and temporary variable has its own unrelated type. The types are then \textit{constrained} as a result of discovering the ways in which they are related from the protocol definition. For example, the presence of a term \code{Eq(A,B)} `collapses' the types of~$A$ and~$B$ into one equivalence class. Still, at this level, types are purely symbolic. 

These symbolic types exist only in the first phase of the code generator. They are not only resolved prior to phase two, but they are not present in that phase at all. Our imperative form does not deal with the complexity of symbolic (ie.\ generic or parametric) types. Instead, the Reo compiler relies on the Rust idiom of relying on \textit{dispatch} to resolve concrete types at the last possible moment: at the call site. To achieve this result, Reo generates the \code{ProtoDef} object wrapped in a \textit{wrapper function}, whose role is primarily to expose generic type parameters for instantiation to the caller, and construct a \code{ProtoDef} instance within the body, to be invoked with concrete types. Listing~\ref{listing:type_resolve} gives an example of how this generic function appears to the end user for some trivial protocol. Observe how \code{protocol\_1} relies on generics to let the caller, \code{main\_1} which type to select for~\code{T}. The \code{TypeInfo} structure represents a port's data type as data. The details of this type are provided in Section~\ref{sec:type_reflection}.


\begin{listing}[ht]
	\centering
	\inputminted[]{rust}{type_resolve.rs}
	\caption[Concrete vs.\ generic protocol-building functions.]{Comparison between concrete and generic function definitions for building \code{ProtoDef} structures. \code{new\_proto\_a} uses the concrete \code{String} type, and will be compiled to a single function as expected. Reo uses the approach of \code{new\_proto\_b}, which determines the choice of the generic type on demand at the \textit{call site}.}
	\label{listing:type_resolve}
\end{listing}

Unlike Java, Rust makes very few promises about the properties of some generic type. It cannot be certain that instances of some generic type~\code{T} will have a defined operation for checking equality, or for safe \textit{value replication}. To facilitate \textit{useful} polymorphism, Rust relies on \textit{type constraints} to act as a contract between caller and callee: the caller will ensure that only types which satisfy the bound may be selected for the type parameter, and the callee is able to interact with its generic arguments in accordance with \textit{behavior} the bounds guarantee the type will define. This approach may be familiar to programmers of Java or C, where this concept manifests as interfaces and declarations (usually in header files) respectively. 

The Reo backend cannot guarantee the generated Rust code is sound unless it is careful to add the necessary type bounds to its generic arguments. The internal representation is inspected for cases which will necessitate the use of specialized operations (such as replication) and will annotate its generic types with \textit{trait bounds}. Ultimately, the resulting coalesced, bounded type parameters are reflected in the generated code as part of the function declaration. The second phase of the code generator can rest assured that for every specialized operation inserted, Reo will have already anticipated the need to guarantee the bound is satisfied. Listing~\ref{listing:type_resolve2} gives an example of a generated trait bound for the case where two memory cells are related by being the same, and requiring the definition of an equality operation.

\begin{listing}[ht]
	\centering
	\inputminted[]{rust}{type_resolve2.rs}
	\caption[Reo-generated trait bounds for generic types.]{Reo-generated \code{ProtoDef} building functions with a generic type~\code{T}. Reo inserts trait bounds to ensure the type chosen for~\code{T} has all the needed behavior. In this case, \code{T:~Eq}, ensuring that instances can be checked for equality, as necessitated by the instruction on line~9. Line~16 generates a compile-time error, as~\code{Foo} does not meet the requirements.}
	\label{listing:type_resolve2}
\end{listing}


\subsubsection{Initial Protocol State}
The initial state of a protocol object is unusual in that it is included in the textual Reo protocol definition, but omitted from the \code{ProtoDef}. This design choice reflects how a protocol's initial state is unassuming at first glance, but is significantly different from the rest of the protocols definition: it is the only part of specification that cannot generally be replicated. Rules and name definitions describe \textit{behavior}, and do not involve any elements of the port's data domain directly. By contrast, a protocol's initial state does not \textit{describe} data, it \textit{is} data. By extracting this facet of the specification, \code{ProtoDef} becomes pure in its role as a \textit{blueprint} for a protocol's behavior. 

Nevertheless, textual Reo is able to specify the initial states of memory cells. To support this functionality, the Reo compiler itself generates code which \textit{builds and injects} the initial values for any initially-filled cells in a controlled environment. To mirror the textual descriptions that define the initial values, these memory types acquire a final trait bound such that their initial values can be constructed per protocol object instantiation \textit{within} the confines of the function that the Reo compiler generates. Listing~\ref{listing:mem_init} exemplifies a trivial protocol where~$A$ is a memory cell defined as being initialized by some value represented by the string `\texttt{hello}'. The generated code inserts a trait dependency, ensuring that the type parameter~\code{T} defines this operation. Observe how now the return result is no longer~\code{ProtoDef}, but rather~\code{ProtoHandle}. While the former represents a re-usable \textit{blueprint} for instantiating a~\code{ProtoHandle}, the latter represents an instantiated protocol, initialized and ready to run; the only user-facing means of generating a new object of the same protocol is to again invoke~\code{protocol}.

\begin{listing}[ht]
	\centering
	\inputminted[]{rust}{mem_init.rs}
	\caption[Reo-generated builder function, returning a protocol instance.]{\code{new\_protocol} instantiates a runnable protocol object by internally building a \code{ProtoDef} description, and then immediately instantiating it with a \code{MemInitial}, whose contents are constructed from parsed strings. In this manner, Reo can control the initialization of protocol instances for any suitable type~\code{T}. Note that \code{\&str} is the immutable reference type of a sized string slice, while \code{String} is an owned, mutable character buffer. Both are common types from the Rust standard library.}
	\label{listing:mem_init}
\end{listing}


\subsection{Phase 2: Rust Library}
\label{sec:translation_phase_2}
Our work follows the precedent set by the Java code generator in relying on a library in the target language to define the lion's share of the behavior for our runtime protocol objects. For Rust, these definitions are bundled into the \textbf{Reo-rs} library, which is added as a dependency to the code generated by the Reo compiler's backend. Chapter~\ref{sec:protocol_runtime} explores the architecture and behavior of our executable protocol objects in detail. For now, it suffices to say that our approach is to represent executable protocols as extensively preprocessed \textit{data structures} which then drive the behavior of a lightweight \textit{interpreter} at runtime. This data-representation is often called \textit{commandification}~\cite{nystrom2014game}.

\subsubsection{Soundness Check}
Our backend is novel in that the work of constructing the executable protocol object requires crossing an API-boundary. Rather than trusting the well-formedness of the Reo-generated \code{ProtoDef}, Reo-rs will check that its input is internally-consistent. By adding these checks, the dependency between the Reo compiler and Reo-rs is \textit{unidirectional}; users are free to safely construct protocol objects using Reo-rs in their applications directly. The most obvious advantage to this approach is an additional layer of safety, allowing for the compiler and Reo-rs to be maintained separately, eg.\ if the compiler acquires a bug from a new update, the error cannot propagate into Reo-rs unnoticed. Another advantage is the avenues for future work this opens up. Our approach treats protocol structures as data, facilitating their mutation at runtime, resulting in \textit{dynamic protocol reconfiguration}, although exploring this further is beyond the scope of this work.

In native Rust, the usual variable scoping rules apply to ensure that a symbolic identifier is resolved to a meaningful memory position. These systems are so familiar to us that we usually take the complexity of their work for granted. Rules that are second nature to us require explicit enforcement to replicate the work of the checker. As \code{ProtoDef} \textit{commandifies} the behavior to be later executed, the Rust compiler does not interpret these actions in the normal way, and we must mimic its behavior manually. We take this idea a step further by relying on the \code{ProtoDef}'s \textit{premise} to facilitate a mechanism that mimics the Rust \textit{borrow checker} system; rules trace which variables are \textit{valid} (ie.\ initialized) throughout the rule's execution top to bottom, tracking changes as a result of actions filling or swapping their values. In this manner, we are able to catch invalid memory accesses during \code{build}, rather than encountering them at runtime. An additional perk of mimicking this system is our ability to detect the occurrence of values which \textit{must} be consumed during the rule's firing, but whose consumption is not included in the specification. For example, an \textit{imperative rule} may include some putter port $P$ in its ready-set (and thus, its synchronization constraint), but associate no \textit{movement} with $P$'s value. A na\"ive implementation which overlooks such occurrences may introduce \textit{memory leaks} for such cases if it takes the specification at face value. Instead, our custom borrow checker will reach the end of of the specified actions and conclude that as $P$ was not explicitly emptied, it will insert a trivial movement $P\mapsto{} \{\}$ to ensure the value is consumed. This is analogous to how Rust's borrow checker inserts \code{drop} calls to destroy local variables which go out of scope unconsumed. By performing this extensive checking, Reo-rs affords an expressive \code{build} function, capable of giving detailed error information in response to an invalid input. The signature of this function is given in Listing~\ref{listing:build}.


\begin{listing}[ht]
	\centering
	\inputminted[]{rust}{build.rs}
	\caption[TODO.]{Signature of the~\code{build} function. Its inputs are (1) an immutable reference to a \code{ProtoDef}, which is used to determine the protocol's behavior, and (2) a \code{MemInitial}, which stores initialized memory cells to be incorporated into the protocol's state. The return result is an enumeration type, returning \code{ProtoHandle} upon success, and a tuple on failure, whose elements are, respectively (1) the index of the imperative rule where the error occurred if applicable, and (2) another sum type, communicating the nature of the error with additional information. }
	\label{listing:build}
\end{listing}

By restricting our API such that all executable protocol objects are \textit{only} created by \code{build}, our runtime interpreter is able to rely on the properties we guarantee and avoid checking them itself. In this way, checking for soundness is also an optimization.


\subsubsection{Protocol Initialization}
As far as Reo is concerned, the entry point to Reo-rs is the \code{build} method, which uses a \code{ProtoDef} as a read-only `blueprint', and consumes a \code{MemInitial} object to build and initialize the state of \code{Proto} instance on the heap. This object is returned indirectly, via a \code{ProtoHandle}, such that access the \code{Proto} instance can be shared by \textit{cloning} the handle. Listing~\ref{listing:mem_init} demonstrates how this appears to the caller.

\code{ProtoDef} and \code{Proto} are very similar structures, which both correspond closely to the language-agnostic definition of \textit{imperative form} given in Section~\ref{sec:imperative_form_definition}. However, they represent the same information differently as they have different purposes.

\noindent{}
\begin{footnotesize}
	\begin{tabular}{l|p{50mm}p{60mm}}
		& \code{ProtoDef}  & \code{Proto} \\
		\hline
		purpose & Defines a protocol's behavior. & Is efficiently interpretable at runtime to execute the defined behavior. \\
		identifiers & Symbolic strings. & Indices, keys and pointers directly into data structures. \\
		redundancy & Minimizes redundancy for brevity and to minimize error surface. & Replicates data to optimize for access time. \\
		state & Is stateless, but describes protocol state. & Is stateful. Contains conventional protocol state in memory cells, and meta-state for `bookkeeping' thread access, ie.\ stateful mutex locks.
	\end{tabular}
\end{footnotesize}
\vspace{1em}

\code{build} initializes the state of the runnable protocol object, \code{Proto}. Reo-rs defines behavior for bringing these types to life at runtime, complete with granular synchronization primitives and various optimizations. Thus, \code{build} completes the final sub-task of code generation, $T_{run}$. Chapter~\ref{sec:protocol_runtime} to follow details the properties of these runnable protocol objects, including how they are represented and how they behave to act as coordination mediums according to their \code{ProtoDef} specifications.
