\chapter{Protocol Translation}
\label{sec:imperative_form}
In this chapter, we describe the process of translating the Reo compiler's internal representation of a protocol specification into an executable \textit{protocol object} in the Rust language.

\section{Two-Phase Code Generation}
In this section, we explain and motivate our approach of segmenting the code generation process into two distinct phases. Throughout, we refer to the precedent set by the existing Reo compiler backend for generating code in the Java language, as it has seen the language most similar to Rust which has seen significant development.   

\subsection{Generation Sub-tasks}
\label{sec:sub_tasks}
Reo specifications represent connectors declaratively as relations between ports. They are thus well-suited to reasoning about the protocol's properties. In contrast, our target imperative languages such as Java and Rust represent computation such that it corresponds more closely to machine instructions; they are imperative, laying out sequences of actions which together emerge as interaction at runtime. Where interactions in the former can be oriented around the synchronous observations of port values, interactions of the latter must be expressed as sequences of actions, laid out over time. Implementing algorithms for translating between these forms must take care that the translation procedure between these forms preserves the semantics as intended; choosing the incorrect ordering can change the nature of the emergent interaction in unexpected ways. For example, reading memory cell \textit{before} writing it corresponds to a different interaction than reading it \textit{after} writing it. 

Java, Rust and Reo have in common that they are strongly-typed languages. Reo's specifications are permitted a degree of \textit{type elision}; for the sake of programmer ergonomics, the data-types of ports may be omitted, such that they can later be derived in context. Rust shares this property, and so the Rust compiler works to \textit{resolve} data types during compilation. Circumstantially, these elisions may produce cases for which a correct resolution is impossible, as the type annotations or constraints present are in conflict. Our task is to emulate this work ourselves, assigning concrete types for port objects in our emitted code such that is guaranteed to type-check in the Rust language. Failure to do this correctly would result in Reo emitting code rejected by the Rust compiler. This would not be a threat to correctness, but it results in significant inconvenience to the programmer.

Regardless of the intermediate representation, protocol objects must ultimately be emitted in the target language. Aside from expression in the correct syntax, the end result must make explicit any work required to make it \textit{executable} with the desired runtime behavior. Even simple concepts require the support of auxiliary book-keeping structures to maintain the protocol object's state, and specialized \textit{concurrency primitives} are needed to ensure that actions compose into interactions at runtime in the expected way. Clearly, this is very particular to the target language, as they vary greatly on how they fundamentally express operations on data at a granular level.

In summary, we identify and name three sub-tasks of generating target language protcol objects from a Reo protocol description:
\begin{itemize}
	\item [$T_{seq}$] Declarative interactions must be decomposed into sequences of imperative actions.
	\item [$T_{type}$] Ports must be given data types such that they agree with any type-annotations in the Reo specification, and successfully type-check in the target language.
	\item [$T_{run}$] Details necessary to make the result runnable are included. Symbolic actions are represented as concrete operations on data.
\end{itemize}


\subsection{Decoupling the Reo Compiler from Rust}
\label{sec:decoupling_reo_rust}
The Reo compiler has an existing backend for generating Java code. It works by generating Java according to the structure of a \textit{template generator}. In this manner, it can be thought of as performing all code-generation sub-tasks at the same time directly from the compiler's intermediate representation. However, the extent to which the Reo compiler is \textit{coupled} to the Java language is reduced through the reliance on a Java library for the granular implementations of structures that are common to all protocol objects; rather than generating these classes each time, the Reo compiler simply generates a dependency. For example, the library defines a \code{Component} interface, for which the code generator produces a protocol-specific implementor class. Consequently, a significant part of the $T_{run}$ sub-task (sub-tasks are defined in Section~\ref{sec:sub_tasks}) is delegated to this library.

For $T_{type}$, help comes from the Reo compiler itself, which in its current form was developed with support for Java in mind. This is visible in its internal representation. For example, types for which no explicit data type annotation was included are assigned the \code{Object} type, which encapsulates all types that may be concretely chosen for transmission through ports. This design essentially uses \code{Object} as an all-encompassing \textit{sum type}, relegating the task of \textit{type reflection} (determining concretely which `variant' of \code{Object}) to the user themselves. This approach is sufficient in the case of Java, as \code{Object} supports all the behavior relied about by the Reo protocol object at runtime, namely (1) data-equality checks, (2) data movement, and (3) data replication\footnote{In the chapter to follow we discuss how this approach introduces safety concerns. In a nutshell, Reo-generated Java assumes that the replication of \code{Object} references preserves Reo's value-passing semantics. This is not the case, as it may result in \textit{data races}.}.

Only $T_{seq}$ is performed almost entirely by the template generator. For simple protocols, this task is relatively easy, as there is not much to add when actions are largely concurrent. For example, replicating the contents of a memory cell into a set of others is simply-done in Java by first reading an object reference, and then overwriting the others one at a time. However, ordering dependencies must be resolved very carefully in the general case. The current Java code generator is susceptible to erroneously observing value $x$ at memory cell $M$ in the event that the observation is synchronous with $M$'s value being overwritten by $x$. Even with the help of the template generator, this translation is sufficiently complex to make detection of these bugs difficult.

Rust is able to mimic Java's approach to create a similar backend through the explicit use of \textit{dynamic dispatch}, such that types can be collapsed to something analogous to Java's \code{Object} class. If done na\"ively, the resulting backend would inherit the problems of its predecessor, and new ones to boot; the Java-like approach is not idiomatic in the context of Rust; it would not make good use of the extensive control of systems resources unique to a systems language. Chapter~\ref{sec:protocol_runtime} to follow goes into detail about the properties of the protocol runtime. Here, it suffices to say that we wish to implement a runtime that does not rely on heap-allocation of its port-values, and thus cannot rely entirely on dynamic dispatch. Furthermore, our runtime wishes to perform more extensive optimizations, relying on the unique abilities of our systems language to manipulate its resources at a low level. All these extensions pose a problem in particular for $T_{run}$, as runtime properties directly influence how the executable protocol objects are represented. Our work in unremarkable in its solution to this problem: we delegate~$T_{seq}$ to a Rust library. However, we make this separation more extreme. In a nutshell, we wish to partition the work of code generation into two clear \textit{phases}, the former of which performs tasks~$T_{seq}$ and~$T_{type}$, and the latter of which performs $T_{seq}$. To minimize coupling between the modules performing these tasks, the interface between phases is made terse, unambiguous and explicit in the definition of a new intermediate representation of protocol connectors: the \textit{imperative form}.

\subsection{Temporary Simplifications}
Our intention is to isolate the Reo backend from Rust's specifics as extensively as possible. In this manner, we decouple the modules responsible for the code-generation subtasks in accordance with good software engineering practices. Furthermore, it facilitates the \textit{re-use} of the first phase of the code generation process for \textit{other} imperative programming language targets. The section to follow defines imperative form to be as target-language agnostic as possible. However, for the sake of minimizing the disturbance to the Reo tooling ecosystem, we still embrace the per-target structure for Reo code generation for now. As such, the Reo compiler still specifies a Rust language target, and emits executable Rust source as a result. Our representation of the \textit{imperative form} is expressed in Rust syntax (as the \code{ProtoDef} type) such that this reliance on an intermediate representation is invisible to the end user. As far as they are concerned, Reo generates native Rust that just happens to \textit{somehow} make minimal use of Rust-specific syntax. \code{ProtoDef} corresponds closely to the definition of imperative form, facilitating this decision being overturned in future with minimal effort.


\section{Imperative Form}
\label{sec:imperative_form_sec}
In this section, we define our new intermediate representation of Reo protocol specifications. We include an intuitive look at how it captures the details of the Reo compiler's internal representation, but such that only $T_{seq}$ remains to be performed before the finished Rust source code can be emitted.


\subsection{Concept}
Imperative form represents a protocol whose translation from Reo to an imperative language has been completed as fully as possible, but stopped short of introducing implementation- and language-specifics. Thus it is still a specification, free from particular syntax, and rendered in terms of \textit{symbolic} identifiers and data types to be resolved in the manner befitting the target language. In terms of the generation sub-tasks defined in Section~\ref{sec:sub_tasks}, imperative form represents the completion of~$T_{seq}$ and~$T_{type}$, but not~$T_{run}$.

The translation from Reo's internal representation to imperative form is \textit{lossless}, and so any language compiling from the former would also be able to compile from the latter. However, the utility of this representation is the increase in \textit{explicitness}, capturing the result of significant preprocessing toward some imperative target. As the fine details of these languages differ, imperative form is not a perfect fit for them all to the same extent. Languages are best-served if they align with the following assumptions introduced during~$T_{seq}$\footnote{Imperative form assumes that the target language can assign static data types to ports. However, this assumption is shared by Reo itself, and does not present a problem for untyped languages. For imperative languages without types, ports can simply share some~\code{Any} type, satisfying this assumption trivially.}, and reflected in the imperative form itself:
\begin{enumerate}
\item The value of a variable cannot be read before it is computed.
\item Actions can be ordered into a sequence of two sub-sequences $S_a$ and $S_b$, such that $S_a$ does not result in \textit{observable effects} beyond the protocol.
\end{enumerate}

\subsubsection{Rules as Transactions}

The Reo compiler's internal representation partitions the work of a rule into its \textit{guard} and \textit{assignments}. This is already a step in the direction of imperative computation, observing that some work (the guard) must be performed \textit{prior} to deciding whether the rule \textit{fires}, in which case the assignment follows. As the protocol does not define the moments when it will evaluate the guard, it is necessary that this evaluation has no side effects ie.\ observable effects to the outside world. Assumption 2 generalizes this notion, representing the sequence of an interaction's constituent rules broken into subsequences $S_a$ and $S_b$, corresponding to guard and assignment respectively.

We elaborate our intuition by noting that mutations of the protocol's state do not result in observable effects as long as the effect is \textit{reversed} before becoming observable. We are therefore able to support operations in $S_a$ that manipulate persistent variables so long as there is a well-defined means of being reversed. Ultimately, we are able model imperative form rules as \textit{transactions}, where the instant of \textit{commitment} separates $S_a$ from $S_b$. Reversible actions occur in $S_a$, and persistent actions occur in $S_b$. At any time prior to commitment, a special \textit{meta-action} in $S_a$ can initiate a \textit{rollback}, reversing the effects of transient actions and aborting.

\subsubsection{Action Granularity}

Imperative form represents a protocol's defined interaction as actions to be computed in the specified sequence. At this stage, our representation is still symbolic; actions do not necessarily correspond 1-to-1 with those in the target imperative language\footnote{Clearly this is impossible, as different languages express the same work at a different granularity of actions.}. How the these symbolic rules are representing in the final language is not specified. Consequently, imperative form walks the fine line between between preprocessing and under-specification with the choice of the granularity of its actions; the finer the granularity, the more (over-)specified the action sequence. 

From assumption~2, imperative form can be understood to represent any rule with \textit{at least} two actions, in which case $S_a$ and $S_b$ are singleton sequences such that the rule's assignment ($S_b$) follows the evaluation of the rule's guard ($S_a$). Further elaborations of ordering are introduced as a consequence of assumption 1; actions are fragmented as much as necessary to ensure that the creation of any \textit{temporary variables} occur before their values are ever \textit{accessed}. For simple protocols involving no temporary variables, assumption 1 does not come into play and the resulting imperative form corresponds closely to the Reo compiler's internal representation.

Complex connectors require more ordered actions to represent the computation and evaluation of temporary variables. For example, consider a protocol in RBF with data constraint $X=f(X)$ and synchronization constraint $\{X\}$ with only input (putter) port~$P$. This rule can be understood as ``$X$ fires \textit{if} the results of function~$f$ on its put-value is equivalent to the value itself''. Here, the result of $f$ clearly cannot be inspected until \textit{after} it is computed. The imperative form representation includes specifies that a temporary variable $f_X$ be computed and stored. By assumption~1, the decision of firing these rules must follow the creation of these variables, clearly putting these actions in $S_a$. This matches our intuition: the guard may be unsatisfied, requiring that the creation of the temporary variables be reversed, ie.\ the temporary values are discarded silently.

A final elaboration on the sequence of actions is necessary to account for every memory cell whose value~$x$ is synchronously read and overwritten. Reo's internal representation inherits syntax from \textit{temporal logic} to disambiguate reads before and after overwrites for such cases: $m$ and $m'$ represent the current and `next' values respectively, with overwriting occurring in-between. The common solution to such problems in an imperative context is to first \textit{move} the value to a temporary variable. To this end, we define an action for \textit{swapping} the contents of two memory cells (which may be empty or full). Such an action is intuitive for most imperative languages, and can be trivially \textit{reversed} such that it may be included in $S_a$ if needs be.

\subsubsection{Data Movement}
The Reo compiler's internal representation represents all a rule's \textit{observable effects} as a set of assignments. Equivalently, assignments are a partial mapping from \textit{destination} (any persistent location able to accept values ie.\ \textit{getter} ports or empty memory cells) to their assigned value. Imperative form has in common that it represents all observable effects as these assignments in this concurrent fashion. Concretely, $S_b$ is always defined as a single, coarse-grained action specifying the destination of values in the result of the rule firing. However, it represents this mapping turned on its head; expressed as a mapping from \textit{source} (a set of values) to sets of destinations. This representation is chosen for its orientation making more easy to track the movement of each datum. The intuition is that this representation makes it easier to treat values as \textit{resources} that potentially require delicate management. Depending on the nature of the target language, it may be necessary to check the \textit{affinity} (see Section~\ref{sec:affine_type_systems}) of these values, as it becomes of paramount importance to control the number of destinations per value. As the imperative form is language-agnostic, it makes no attempt to rectify this problem itself, relying on the $T_{run}$ task to resolve the problems in the manner befitting the target language. As an example, the Rust language is concerned with the \textit{affinity} of some its data types, and must explicitly create new affine resources with an explicit \code{clone} operation. This is discussed further in Section~\ref{sec:translation_phase_2} to follow.

This movement representation also makes clear the cases for which a value goes \textit{unused}. \textit{Relevant} or \textit{linear} type systems may wish to reject such protocols, or insert explicit operations to handle the equivalent effect, as their definition forbids the destruction of data elements\footnote{For example, a linear imperative language may wish to simulate data destruction by injecting the special \code{Destroyed} destination for otherwise empty destination sets.}. Languages with explicit \textit{memory management} may need to address the cases of data destruction with the injection of special handlers. For example, the C language may necessitate the injection of a \code{free} call to avoid leaking memory.

\subsection{Definition}
\label{sec:imperative_form_definition}
A protocol description in imperative form must be provided by a single structure which provides mappings for symbolic names such that they can be resolved as they are encountered when traversing the rest of the definition. This \textit{name definition} structure is similar to a \textit{symbol table}. The behavior of the connector itself is defined by a set\footnote{In our implementation, these rules are provided in an ordered list, primarily for the purpose of making for more comprehensible error messages.} of \textit{imperative rules}, each corresponding to an RBA rule, given by a tuple $(P, I, M)$ with:
\begin{enumerate}
	\item \textbf{Premise $P$}\\
	A tuple of three \textit{identifier} sets $(P_R, P_F, P_E)$. $P_R$~is the \textit{synchronization constraint}, ie.\ the set of ports identifiers whose values must be `ready'. $P_F$ and $P_E$ are the sets of \textit{memory variables} which must be known to be full and empty respectively, such that it is known whether they can be read or written from. The rule can certainly not consider firing unless all ports are ready and all memory cells are in the specified states.
	
	\item \textbf{Instructions $I$}\\
	A list of reversible \textit{instructions} which are performed in sequence. These instructions have no immediately observable effects, such that they can be reverted in the event of a \textit{rollback}. Concretely, each instruction is one of:
	\begin{itemize}
		\item $check(p)$\\
		Trigger a rollback if predicate $p$ over data is satisfied.
		\item $fill_P(m, p)$\\Fill an empty memory variable $m$ with the result of a predicate $p$ over available data. The value's data type is implicitly \textit{boolean}.
		\item $fill_F(m, f, a)$\\
		Fill an empty memory variable $m$ with the result of invoking function $f$ with parameters $a$, a list of references to data variables with length matching the arity of $f$. It is incorrect for $f$ to \textit{mutate} its arguments, as this would result in observable effects which cannot be rolled back.
		\item $swap(m_0,m_1)$\\
		Swap\footnote{In principle, any reversible data-agnostic manipulation is possible, but swapping values is sufficiently expressive and intuitive for our purposes.} the values in two memory variables~$m_0$ and~$m_1$.
	\end{itemize}
	If a rollback is triggered by $check$, any swapped memory cells are swapped back, and any memory cells whose values were created by $fill_P$ or $fill_F$ are destroyed.
	
	\item \textbf{Movements $M$}\\
	A mapping from \textit{resources} to any identifiers that can act as getters (getter ports and empty memory cells). This represents the \textit{observable effects} of the rule firing after instructions are performed without triggering rollback. 
\end{enumerate}

As an example to demonstrate this representation, the RBA rule in the previous section with data constraint $X=f(X)$
and synchronization constraint $\{X\}$ can be represented in the imperative-form rule with:

\noindent{}
\begin{tabular}{r|l}
	rule element	&  value \\ \hline
	premise	&  $(\{X\}, \{\}, \{f_X\})$ \\
	instructions	& $[fill_F(f_X, f, [X]), \quad{} check(X = f_X)]$ \\
	movements	& $\{X \rightarrow{}\emptyset{}, f_X \rightarrow{}\emptyset{}\}$ 
\end{tabular}
\vspace{1em}

\section{Translation Pipeline}
In this section we explore the translation procedure of protocols from the Reo compiler's internal representation (`IR'), through our intermediate imperative form (`IF'), ultimately resulting in a representation executable in Rust. As explained in Section~\ref{sec:decoupling_reo_rust}, this process is performed as the sequence of two distinct phases with IF acting as the intermediary in-between. Throughout this section, we refer to the generation process in terms of its three distinct subtasks $\{T_{seq}, T_{type}, T_{run}$, defined in Section~\ref{sec:sub_tasks}.

%\subsection{Temporary Simplifications}
%IF includes explicit annotations for the data types of ports, memory cells and temporary variables, representing the completion of $T_{type}$. Our definition thus-far reflects our conceptual design, where the translation to IF is entirely target-language agnostic. In our current implementation, this is not strictly true; our translation to IF is not entirely free of Rust for one purpose only: currently, Rust is the only target language making use of IF. For the purpose of minimizing the (uninteresting) implementation work for the project, the Reo compiler emits rust source directly, which is a thin, Rust-friendly wrapper over a data structure of a protocol specification in IF as one would expect. This simplification has three advantages:
%
%\begin{enumerate}
%	\item We trivialize the burden on the second translation phase by representing IF in a form the Rust compiler understands inherently: a type in the Rust language. 
%	\item We can rely on Rust's \textit{generic types} to generate Rust dependency which is able to delay the resolution of its symbolic data-types (part of $T_{run}$) to the last possible moment, relying on a representation that is both idiomatic for the language, and trivial to implement: we represent them as Rust \textit{generic types}.
%	\item We do not deviate from the current idiom in the Reo compiler, whereby a target language generates a dependency for the source language directly.
%\end{enumerate}
%
%Effectively, our current implementation of Reo's Rust backend emits a Rust-specific dependency which relies almost entirely on its IF intermediate form in the manner described in Section~\ref{
}

\subsection{Phase 1: Reo Compiler Backend}
\label{sec:translation_phase_1}
We extend the Reo compiler with a backend for translating the compiler's internal representation (`IR') to imperative form (`IF'). Section~\ref{sec:decoupling_reo_rust} explains how this task involves two of the three sub-tasks for generating to a particular imperative language target, namely~$T_{seq}$ and~$T_{type}$. 

\subsubsection{Action Sequencing}
$T_{seq}$ necessitates transforming a each of the protocol's rules into a sequence of symbolic actions. The most significant work occurs as a result of how differently \textit{values} are represented. IR is rather declarative, representing the result of a rule's firing as an \textit{assignment}, mapping \textit{destinations} (getter ports and empty memory cells) to \textit{terms}. Compared to RBF, IR already represents a significant transformation in isolating these values on a per-destination basis. 

Our translation procedure initializes all three fields $\{P, I, M\}$ of an imperative rule as initially-empty, and populates them incrementally by recursively traversing the IF representations \textit{assignment} mappings. The term at each level is understood as some \textit{value}, either some primitive (ie.\ the value \textit{put} by a port or the contents of a filled memory cell), the result of a \textit{function invocation}, or the evaluation of some boolean expression. Terms correspond with values, and each encountered in this manner is associated with an \textit{identifiers}. 
Primitives are the vast majority in practice, and are simply associated with the identifiers of their port or memory cell. All others are assigned fresh \textit{temporary variable} identifiers, their order in the collection reflects the order in which they were encountered. 

The most na\"ive means of translating IR to IF is achieved by inserting instructions into~$I$, creating all temporary variables, careful to order these instructions such that their corresponding terms always involve created values, ie.\ leaf terms occur above their roots in~$I$. The final instruction is a single monolithic \textit{check} instruction, which triggers rollback at the last possible moment, safe in the knowledge that any values accessed during evaluation already have been created. Finally,~$M$ is populated with mappings of movements; every identified value is included, with a \textit{destination} for every port or memory cell in whose assignment-term the value occurred as mapped the root term. Any putter-ports in the synchronization constraint that never occur in terms are provided with trivial mappings in $M$ to ensure their values are acquired and discarded correctly.
A final elaboration is necessary for every memory cell that is both read and written to. This occurs whenever it occurs both as a \textit{key} in the assignment map (it is assigned to, ie. written) and as a subterm of some value (ie.\ its value is accessed directly or indirectly in an instruction, eg.\ it occurs as an argument to a function call, computing a temporary value). For each such memory cell~$q$, $I$ is prefixed with a $swap(q, old_q)$ instruction, where $old_q$ is fresh, initially empty temporary variable. For all instructions to follow, and in~$M$, $q$~acts as a getter of the memory cells written value, while $old_q$ acts as a putter corresponding with a value moved to some nonempty set of destinations.

As it was described thus-far, our procedure is able to correctly render any IR rule in IF with the necessary properties. For the sake of minimization or performance at runtime, at least three optimization opportunities may elaborate on this procedure, producing semantically-equivalent results.

\begin{enumerate}
	\item Terms that occur repeatedly within assignments throughout the same IR rule may have their \textbf{values deduplicated} by assigning them all the same \textit{identifier}. Care must be taken to ensure that the instruction to create its value is inserted only once, sufficiently early that its creation precedes its \textit{earliest} access. Note that each original occurrence still corresponds with a \textit{destination} in the resultant~$M$ mapping. To clarify, consider the example with getter ports~$A$ and~$B$ both assigned terms corresponding to $f(C)$ where $f$ is some function and $C$ is a putter port. Here, one temporary variable $f_C$ to store the result of $f(C)$ is sufficient; it is simply moved to two distinct destinations, reflected in the mapping $f_C\rightarrow\{A,B\}$ in~$M$.
	
	\item The large, monolithic \textit{check} instruction that acts as a guard to the rules firing can be fragmented into \textbf{numerous guard instructions}. The utility of this is the ability to re-arrange their ordering. For best results, it is beneficial to move checks as early as possible, such that less work is performed prior and subsequently to a rollback whenever the check \textit{fails} at runtime. To be correct, care must be taken not to move guards so early such they precede the creation of any temporary variables their evaluation accesses. For example, consider an IR 
	whose guard is $A\wedge{}B$, where~$A$ and~$B$ are sub-formulas that reason about sub-terms whose evaluation necessitates the creation of temporary values~$t_A$ and~$t_B$. By fragmenting $check(A\wedge{}B)$ into $check(A)$ and $check(B)$, we are able to move the former such that it follows the creation of $A$, but not of $B$. Effectively, the rule is able to \textit{short circuit} its evaluation at runtime, circumstantially avoiding the creation and destruction of the temporary value identified by~$t_B$.
	
	\item \textbf{Static analysis} of values may conclude that a \textit{check} instruction is a tautology, making it safe to omit. Similarly, the presence of even one contradictory \textit{check} makes it possible to discard the rule entirely. This optimization is particularly useful in combination with optimization (2).
\end{enumerate}

\subsubsection{Type Classification and Constraining}
Our backend performs task $T_{type}$ to generate the IF such that the identifier of every port, memory cell, and temporary variable is assigned a symbolic type annotation, such that:
\begin{enumerate}
	\item the types of identifiers match if they exchange values or are checked for equivalence.
	\item no function has contradictory requirements on the types of its arguments.
	\item identifiers used in a boolean-only context (ie.\ in the root of a \code{Formula}) are boolean.
	\item the type defines all the operations in which it may be involved at runtime. This includes operations for \textit{replication}, checking \textit{equality} of values and so on.
\end{enumerate}

Our backend performs this work in tandem with the work of $T_{seq}$ described in the previous section. Initially, every identifier is assigned s fresh symbolic type with no constraints, representing a data-type unrelated to any other, and having no need of any defined operations. In traversing the IR rules, \textit{constraints} are collected, associating them to the relevant identifiers. Incrementally, \textit{type constraints} are collected, accumulating requirements for the properties of types. For example, the type associated with a value is checked for equality, irrevocably acquiring the \code{eq} constraint, marking the need for it to define an operation to check value-equality. In some cases, a relationship between identifiers causes their types to be \textit{unified}. For example, a data-movement from putter~$P$ to getter~$G$ unifies their types, resulting a new type with the union of their constraints.

Ultimately, an IF constructed with a global \textit{symbol table}, defining the mapping from identifiers to types such that our requirements are satisfied.

%IF expects an explicit type-annotation for the \textit{identifiers} of every port, memory cell and temporary value. As IR cannot be relied upon to create explicit annotations, our backend is reponsible for $T_{type}$, the resolution of all symbolic types for identifiers. As IF is still a language-agnostic specification, these types serve little purpose other than to act as \textit{equivalence classes} and discover which properties these symbolic types much satisfy such that the target language can ensure all the 
%
%Previously referred to as $T_{type}$, the backend must extract data-type information about its ports and memory cells from the compiler's internal representation such that the generated Rust is valid. Our approach is to begin by assuming that every port, memory cell and temporary variable has its own unrelated type. The types are then \textit{constrained} as a result of discovering the ways in which they are related from the protocol definition. For example, the presence of a term \code{Eq(A,B)} `collapses' the types of~$A$ and~$B$ into one equivalence class. Still, at this level, types are purely symbolic. 
%
%These symbolic types exist only in the first phase of the code generator. They are not only resolved prior to phase two, but they are not present in that phase at all. Our imperative form does not deal with the complexity of symbolic (ie.\ generic or parametric) types. Instead, the Reo compiler relies on the Rust idiom of relying on \textit{dispatch} to resolve concrete types at the last possible moment: at the call site. To achieve this result, Reo generates the \code{ProtoDef} object wrapped in a \textit{wrapper function}, whose role is primarily to expose generic type parameters for instantiation to the caller, and construct a \code{ProtoDef} instance within the body, to be invoked with concrete types. Listing~\ref{listing:type_resolve} gives an example of how this generic function appears to the end user for some trivial protocol. Observe how \code{protocol\_1} relies on generics to let the caller, \code{main\_1} which type to select for~\code{T}. The \code{TypeInfo} structure represents a port's data type as data. The details of this type are provided in Section~\ref{sec:type_reflection}.
%
%
%\begin{listing}[ht]
%	\centering
%	\inputminted[]{rust}{type_resolve.rs}
%	\caption[Concrete vs.\ generic protocol-building functions.]{Comparison between concrete and generic function definitions for building \code{ProtoDef} structures. \code{new\_proto\_a} uses the concrete \code{String} type, and will be compiled to a single function as expected. Reo uses the approach of \code{new\_proto\_b}, which determines the choice of the generic type on demand at the \textit{call site}.}
%	\label{listing:type_resolve}
%\end{listing}
%
%Unlike Java, Rust makes very few promises about the properties of some generic type. It cannot be certain that instances of some generic type~\code{T} will have a defined operation for checking equality, or for safe \textit{value replication}. To facilitate \textit{useful} polymorphism, Rust relies on \textit{type constraints} to act as a contract between caller and callee: the caller will ensure that only types which satisfy the bound may be selected for the type parameter, and the callee is able to interact with its generic arguments in accordance with \textit{behavior} the bounds guarantee the type will define. This approach may be familiar to programmers of Java or C, where this concept manifests as interfaces and declarations (usually in header files) respectively. 
%
%The Reo backend cannot guarantee the generated Rust code is sound unless it is careful to add the necessary type bounds to its generic arguments. The internal representation is inspected for cases which will necessitate the use of specialized operations (such as replication) and will annotate its generic types with \textit{trait bounds}. Ultimately, the resulting coalesced, bounded type parameters are reflected in the generated code as part of the function declaration. The second phase of the code generator can rest assured that for every specialized operation inserted, Reo will have already anticipated the need to guarantee the bound is satisfied. Listing~\ref{listing:type_resolve2} gives an example of a generated trait bound for the case where two memory cells are related by being the same, and requiring the definition of an equality operation.
%
%\begin{listing}[ht]
%	\centering
%	\inputminted[]{rust}{type_resolve2.rs}
%	\caption[Reo-generated trait bounds for generic types.]{Reo-generated \code{ProtoDef} building functions with a generic type~\code{T}. Reo inserts trait bounds to ensure the type chosen for~\code{T} has all the needed behavior. In this case, \code{T:~Eq}, ensuring that instances can be checked for equality, as necessitated by the instruction on line~9. Line~16 generates a compile-time error, as~\code{Foo} does not meet the requirements.}
%	\label{listing:type_resolve2}
%\end{listing}


\subsubsection{Initial Protocol State}
The initial state of a protocol object is unusual in that it is included in the textual Reo protocol definition, but omitted from the \code{ProtoDef}. This design choice reflects how a protocol's initial state is unassuming at first glance, but is significantly different from the rest of the protocols definition: it is the only part of specification that cannot generally be replicated. Rules and name definitions describe \textit{behavior}, and do not involve any elements of the port's data domain directly. By contrast, a protocol's initial state does not \textit{describe} data, it \textit{is} data. By extracting this facet of the specification, \code{ProtoDef} becomes pure in its role as a \textit{blueprint} for a protocol's behavior. 

Nevertheless, textual Reo is able to specify the initial states of memory cells. To support this functionality, the Reo compiler itself generates code which \textit{builds and injects} the initial values for any initially-filled cells in a controlled environment. To mirror the textual descriptions that define the initial values, these memory types acquire a final trait bound such that their initial values can be constructed per protocol object instantiation \textit{within} the confines of the function that the Reo compiler generates. Listing~\ref{listing:mem_init} exemplifies a trivial protocol where~$A$ is a memory cell defined as being initialized by some value represented by the string `\texttt{hello}'. The generated code inserts a trait dependency, ensuring that the type parameter~\code{T} defines this operation. Observe how now the return result is no longer~\code{ProtoDef}, but rather~\code{ProtoHandle}. While the former represents a re-usable \textit{blueprint} for instantiating a~\code{ProtoHandle}, the latter represents an instantiated protocol, initialized and ready to run; the only user-facing means of generating a new object of the same protocol is to again invoke~\code{protocol}.

\begin{listing}[ht]
	\centering
	\inputminted[]{rust}{mem_init.rs}
	\caption[Reo-generated builder function, returning a protocol instance.]{\code{new\_protocol} instantiates a runnable protocol object by internally building a \code{ProtoDef} description, and then immediately instantiating it with a \code{MemInitial}, whose contents are constructed from parsed strings. In this manner, Reo can control the initialization of protocol instances for any suitable type~\code{T}. Note that \code{\&str} is the immutable reference type of a sized string slice, while \code{String} is an owned, mutable character buffer. Both are common types from the Rust standard library.}
	\label{listing:mem_init}
\end{listing}


\subsection{Phase 2: Rust Library}
\label{sec:translation_phase_2}



Our work follows the precedent set by the Java code generator in relying on a library in the target language to define the lion's share of the behavior for our runtime protocol objects. For Rust, these definitions are bundled into the \textbf{Reo-rs} library, which is added as a dependency to the code generated by the Reo compiler's backend. Chapter~\ref{sec:protocol_runtime} explores the architecture and behavior of our executable protocol objects in detail. For now, it suffices to say that our approach is to represent executable protocols as extensively preprocessed \textit{data structures} which then drive the behavior of a lightweight \textit{interpreter} at runtime. This data-representation is often called \textit{commandification}~\cite{nystrom2014game}.

\subsubsection{Soundness Check}
Our backend is novel in that the work of constructing the executable protocol object requires crossing an API-boundary. Rather than trusting the well-formedness of the Reo-generated \code{ProtoDef}, Reo-rs will check that its input is internally-consistent. By adding these checks, the dependency between the Reo compiler and Reo-rs is \textit{unidirectional}; users are free to safely construct protocol objects using Reo-rs in their applications directly. The most obvious advantage to this approach is an additional layer of safety, allowing for the compiler and Reo-rs to be maintained separately, eg.\ if the compiler acquires a bug from a new update, the error cannot propagate into Reo-rs unnoticed. Another advantage is the avenues for future work this opens up. Our approach treats protocol structures as data, facilitating their mutation at runtime, resulting in \textit{dynamic protocol reconfiguration}, although exploring this further is beyond the scope of this work.

In native Rust, the usual variable scoping rules apply to ensure that a symbolic identifier is resolved to a meaningful memory position. These systems are so familiar to us that we usually take the complexity of their work for granted. Rules that are second nature to us require explicit enforcement to replicate the work of the checker. As \code{ProtoDef} \textit{commandifies} the behavior to be later executed, the Rust compiler does not interpret these actions in the normal way, and we must mimic its behavior manually. We take this idea a step further by relying on the \code{ProtoDef}'s \textit{premise} to facilitate a mechanism that mimics the Rust \textit{borrow checker} system; rules trace which variables are \textit{valid} (ie.\ initialized) throughout the rule's execution top to bottom, tracking changes as a result of actions filling or swapping their values. In this manner, we are able to catch invalid memory accesses during \code{build}, rather than encountering them at runtime. An additional perk of mimicking this system is our ability to detect the occurrence of values which \textit{must} be consumed during the rule's firing, but whose consumption is not included in the specification. For example, an \textit{imperative rule} may include some putter port $P$ in its ready-set (and thus, its synchronization constraint), but associate no \textit{movement} with $P$'s value. A na\"ive implementation which overlooks such occurrences may introduce \textit{memory leaks} for such cases if it takes the specification at face value. Instead, our custom borrow checker will reach the end of of the specified actions and conclude that as $P$ was not explicitly emptied, it will insert a trivial movement $P\mapsto{} \{\}$ to ensure the value is consumed. This is analogous to how Rust's borrow checker inserts \code{drop} calls to destroy local variables which go out of scope unconsumed. By performing this extensive checking, Reo-rs affords an expressive \code{build} function, capable of giving detailed error information in response to an invalid input. The signature of this function is given in Listing~\ref{listing:build}.


\begin{listing}[ht]
	\centering
	\inputminted[]{rust}{build.rs}
	\caption[TODO.]{Signature of the~\code{build} function. Its inputs are (1) an immutable reference to a \code{ProtoDef}, which is used to determine the protocol's behavior, and (2) a \code{MemInitial}, which stores initialized memory cells to be incorporated into the protocol's state. The return result is an enumeration type, returning \code{ProtoHandle} upon success, and a tuple on failure, whose elements are, respectively (1) the index of the imperative rule where the error occurred if applicable, and (2) another sum type, communicating the nature of the error with additional information. }
	\label{listing:build}
\end{listing}

By restricting our API such that all executable protocol objects are \textit{only} created by \code{build}, our runtime interpreter is able to rely on the properties we guarantee and avoid checking them itself. In this way, checking for soundness is also an optimization.


\subsubsection{Protocol Initialization}
As far as Reo is concerned, the entry point to Reo-rs is the \code{build} method, which uses a \code{ProtoDef} as a read-only `blueprint', and consumes a \code{MemInitial} object to build and initialize the state of \code{Proto} instance on the heap. This object is returned indirectly, via a \code{ProtoHandle}, such that access the \code{Proto} instance can be shared by \textit{cloning} the handle. Listing~\ref{listing:mem_init} demonstrates how this appears to the caller.

\code{ProtoDef} and \code{Proto} are very similar structures, which both correspond closely to the language-agnostic definition of \textit{imperative form} given in Section~\ref{sec:imperative_form_definition}. However, they represent the same information differently as they have different purposes.

\noindent{}
\begin{footnotesize}
	\begin{tabular}{l|p{50mm}p{60mm}}
		& \code{ProtoDef}  & \code{Proto} \\
		\hline
		purpose & Defines a protocol's behavior. & Is efficiently interpretable at runtime to execute the defined behavior. \\
		identifiers & Symbolic strings. & Indices, keys and pointers directly into data structures. \\
		redundancy & Minimizes redundancy for brevity and to minimize error surface. & Replicates data to optimize for access time. \\
		state & Is stateless, but describes protocol state. & Is stateful. Contains conventional protocol state in memory cells, and meta-state for `bookkeeping' thread access, ie.\ stateful mutex locks.
	\end{tabular}
\end{footnotesize}
\vspace{1em}

\code{build} initializes the state of the runnable protocol object, \code{Proto}. Reo-rs defines behavior for bringing these types to life at runtime, complete with granular synchronization primitives and various optimizations. Thus, \code{build} completes the final sub-task of code generation, $T_{run}$. Chapter~\ref{sec:protocol_runtime} to follow details the properties of these runnable protocol objects, including how they are represented and how they behave to act as coordination mediums according to their \code{ProtoDef} specifications.


\subsection{Temporary Simplifications}
TODO