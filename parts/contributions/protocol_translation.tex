\chapter{Code Generation}
\label{sec:imperative_form}







\section{Two-Phase Code Generation}
In this section, we explain and motivate our approach of segmenting the code generation process into two distinct phases. Throughout, we refer to the precedent set by the existing Reo compiler backend for generating code in the Java language, as it has seen the language most similar to Rust which has seen significant development.   

\subsection{Generation Sub-tasks}
Reo specifications represent connectors declaratively as relations between ports. They are thus well-suited to reasoning about the protocol's properties. In contrast, our target imperative languages such as Java and Rust represent computation such that it corresponds more closely to machine instructions; they are imperative, laying out sequences of actions which together emerge as interaction at runtime. Where interactions in the former can be oriented around the synchronous observations of port values, interactions of the latter must be expressed as sequences of actions, laid out over time. Implementing algorithms for translating between these forms must take care that the translation procedure between these forms preserves the semantics as intended; choosing the incorrect ordering can change the nature of the emergent interaction in unexpected ways. For example, reading memory cell \textit{before} writing it corresponds to a different interaction than reading it \textit{after} writing it. 

Rust and Java have in common their strongly-typed language, while  is untyped. To facilitate the desired level of specificity for connectors, the \textit{textual Reo} language has support for optional \textit{data type annotations} on ports, which are retained by the Reo compiler for use by the relevant backend. How these annotations are used is not specified, as their interpretation is only sensible for target languages in a case-by-case basis. To us, these annotations are understood as types which the associated ports must be capable of transmitting. For our typed languages, typing information must be injected such that the emitted target code is valid.

Regardless of the intermediate representation, protocol objects must ultimately be emitted in the target language. Aside from expression in the correct syntax, the end result must make explicit any work required to make it \textit{executable} with the desired runtime behavior. Even simple concepts require the support of auxiliary book-keeping structures to maintain the protocol object's state, and specialized \textit{concurrency primitives} are needed to ensure that actions compose into interactions at runtime in the expected way. Clearly, this is very particular to the target language, as they vary greatly on how they fundamentally express operations on data at a granular level.

In summary, we identify and name three sub-tasks of generating target language protcol objects from a Reo protocol description:
\begin{itemize}
	\item [$T_{act}$] Synchronous interactions must be decomposed into asynchronous actions, with the roles of each participant laid out in a sequence.
	\item [$T_{type}$] Ports must be given data types such that they agree with any (optional) type-annotations in the Reo specification, and successfully type-check in the target language.
	\item [$T_{run}$] Details necessary to make the result runnable are included. Symbolic actions are represented as concrete operations on data.
\end{itemize}


\subsection{Decoupling the Reo Compiler from Rust}
\label{sec:decoupling_reo_rust}
The Reo compiler has an existing backend for generating Java code. It works by generating Java according to the structure of a \textit{template generator}. In this manner, it can be thought of as performing all code-generation sub-tasks at the same time directly from the compiler's intermediate representation. However, the extent to which the Reo compiler is \textit{coupled} to the Java language is reduced through the reliance on a Java library for the granular implementations of structures that are common to all protocol objects; rather than generating these classes each time, the Reo compiler simply generates a dependency. For example, the library defines a \code{Component} interface, for which the code generator produces a protocol-specific implementor class. Consequently, a significant part of $T_{run}$ is delegated to this library.

For $T_{type}$, help comes from the Reo compiler itself, which in its current form was developed with support for Java in mind. This can be seen in the structure of its internal representation. For example, types are given a \textit{default} type in the event the input specification omits a type-annotation; this is no trouble for Java, which can up-cast all classes to~\code{Object} without losing the capabilities on which the protocol relies at runtime, namely (1) equality checks, and (2) object replication\footnote{In the chapter to follow, we discuss the consequence of this approach, as it necessarily assumes that operations for checking equality and replication can be resolved for any types.}.

Only $T_{act}$ is performed almost entirely by the template generator. For simple protocols, this task is relatively easy, as there is not much to add when actions are largely concurrent. For example, replicating the contents of a memory cell into a set of others is simply-done in Java by first reading an object reference, and then overwriting the others one at a time. However, ordering dependencies must be resolved very carefully in the general case. The current Java code generator is susceptible to erroneously observing value $x$ at memory cell $M$ in the event that the observation is synchronous with $M$'s value being overwritten by $x$. Even with the help of the template generator, this translation is sufficiently complex to make detection of these bugs difficult.

Rust is able to mimic Java's approach to create a similar backend through the explicit use of \textit{dynamic dispatch}, such that types can be collapsed to something analogous to Java's \code{Object} class. If done na\"ively, the resulting backend would inherit the problems of its predecessor, and new ones to boot; the Java-like approach is not idiomatic in the context of Rust; it would not make good use of the extensive control of systems resources unique to a systems language. Chapter~\ref{sec:protocol_runtime} to follow goes into detail about the properties of the protocol runtime. Here, it suffices to say that we wish to implement a runtime that does not rely on heap-allocation of its port-values, and thus cannot rely entirely on dynamic dispatch. Furthermore, our runtime wishes to perform more extensive optimizations, relying on the unique abilities of our systems language to manipulate its resources at a low level. All these extensions pose a problem in particular for $T_{run}$, as runtime properties directly influence how the executable protocol objects are represented. Our work in unremarkable in its solution to this problem: we delegate~$T_{act}$ to a Rust library. However, we make this separation more extreme. In a nutshell, we wish to partition the work of code generation into two clear \textit{phases}, the former of which performs tasks~$T_{seq}$ and~$T_{type}$, and the latter of which performs $T_{act}$. To minimize coupling between the modules performing these tasks, the interface between phases is made terse, unambiguous and explicit in the definition of a new intermediate representation of protocol connectors: the \textit{imperative form}.

\subsection{Temporary Simplifications}
Our intention is to isolate the Reo backend from Rust's specifics as extensively as possible. In this manner, we decouple the modules responsible for the code-generation subtasks in accordance with good software engineering practices. Furthermore, it facilitates the \textit{re-use} of the first phase of the code generation process for \textit{other} imperative programming language targets. The section to follow defines imperative form to be as target-language agnostic as possible. However, for the sake of minimizing the disturbance to the Reo tooling ecosystem, we still embrace the per-target structure for Reo code generation for now. As such, the Reo compiler still specifies a Rust language target, and emits executable Rust source as a result. Our representation of the \textit{imperative form} is expressed in Rust syntax (as the \code{ProtoDef} type) such that this reliance on an intermediate representation is invisible to the end user. As far as they are concerned, Reo generates native Rust that just happens to \textit{somehow} make minimal use of Rust-specific syntax. \code{ProtoDef} corresponds closely to the definition of imperative form, facilitating this decision being overturned in future with minimal effort.


\section{Imperative Form}
In this section, we define our new intermediate representation of Reo protocol specifications. We include an intuitive look at how it captures the details of the Reo compiler's internal representation, but such that only $T_{act}$ remains to be performed before the finished Rust source code can be emitted.


\subsection{Intuition}
Imperative form makes explicit the \textit{ordering} of events for which the ordering cannot be arbitrary. In other words, synchronous concurrency can still be expressed, but operations whose correctness relies on it are ordered explicitly. For very simple protocols, imperative form is very similar to the compiler's RBA-like internal representation; they have in common that they partition rules into (1) \textit{guards} which contain the \textit{synchronization-} and \textit{data-constraints}, and (2) \textit{assignments} which mutate the protocl's state and produce observable events such as boundary ports putting and getting. 

Imperative form differs from that of the compiler's intermediate representation the more extensively the protocol makes use of synchronous manipulations of its state, ie.\ the more the protocol's state is mutated \textit{within} a single rule. For example, consider a protocol in RBF with data constraint $f(P_0)=f(P_1)$ and synchronization constraint $\{P_0, P_1\}$ with input (putter) ports $P_{0-1}$. This rule can be understood as ``$P_{0-1}$ fire \textit{if} the results of function~$f$ on their put-values are equivalent''. Here, the results of $f$ clearly cannot be compared until \textit{after} they are computed, but these intermediate values must be \textit{stored} somewhere in the meantime. How can we correctly store these values (mutating the state) \textit{before} we know whether this rule will fire and should thus mutate the state? Our solution is to ensure that these sorts of situations can occur only if these `transient' state-mutations can be \textit{undone} without observable effects; erroneous state mutation is not a problem if it is immediately reverted. Thus, imperative form models a synchronous rule firing as a \textit{transaction} by partitioning its actions on either side of some \textit{commit}-instant. Actions thus then fall into one of two categories:
\begin{itemize}
	\item \textbf{Transient Actions}\\
	Actions \textit{before} committing may only perform mutations of the protocol state that can be \textit{rolled back} reversing their effects. Implicitly, this prohibits any actions that are visible to an outside observer. Actions are also allowed to trigger a rollback and abort.
	
	\item \textbf{Persistent Actions}\\
	Action \textit{after} committing may irrevocably mutate the state and produce observable effects, but may not trigger a rollback.
\end{itemize}

For our example RBF, the check for equality acts as a \textit{guard}, but its computation involves the values put by $P_{0-1}$. To be valid, $f$ is required to be a pure function which cannot be observed to \textit{consume} the values of $P_{0-1}$ (eg.\ by reading them via immutable reference), and is invoked to produce three temporary variables, which can subsequently be checked for equality. If the check fails, all actions \textit{roll back}, discarding our temporary variables and aborting the rule firing. This corresponds to a guard that was not \textit{satisfied}. On the other hand, if the equality passes, the rule \textit{commits} and the values of $P_{0-1}$ are irrevocably consumed (and in this case, discarded). In this way, $P_{0-1}$ only observe their data being consumed if the rule fires.

Conceptualizing an imperative form rule purely as action-sequences gives good intuition for the intention behind the representation. However, this would require concurrent events to be sequentialized. However, many actions are not given in the compiler's internal representation in this form. For example, the ordering of assignments is left unspecified. There is no harm in sequentializing these actions if the end result is indeed sequential. For example, the Java protocol object indeed assigns to memory cells sequentially. Rather than assuming this will always be the case\footnote{Indeed, our final implementation dispatches data movements sequentially, but they are completed concurrently. See Section~\ref{sec:data_exchange}.}, we represents persistent actions as \textit{concurrent} movements from sources to destinations. In the same manner, is it unnecessary to sequentialize the observations in the \textit{guard}; it does not matter the order in which ports are checked for availability. For this reason, imperative form is ultimately represented with three distinct structures (1) unordered transient actions performed before (2) ordered transient actions with rollback, and finally (3) unordered persistent actions.


\subsection{Definition}
\label{sec:imperative_form_definition}
A protocol description in imperative form must be provided by a single structure which provides mappings for symbolic names such that they can be resolved as they are encountered when traversing the rest of the definition. This \textit{name definition} structure is similar to a \textit{symbol table}. The behavior of the connector itself is defined by a set\footnote{In our implementation, these rules are provided in an ordered list, primarily for the purpose of making for more comprehensible error messages.} of \textit{imperative rules}, each corresponding to an RBA rule, given by a tuple $(P, I, M)$ with:
\begin{enumerate}
	\item \textbf{Premise $P$}\\
	A tuple of three \textit{identifier} sets $(P_R, P_F, P_E)$. $P_R$~contains the set of identifiers whose values must be `ready', and are thus in stable state, involved in the rule. The subset of identifiers belonging to ports thus encodes the \textit{synchronization constraint} of the associated RBA. $P_F$ and $P_E$ are the sets of \textit{memory variables} which must be known to be full and empty respectively, such that it is known whether they can be read or written from. The rule can certainly not consider firing unless all ports are ready and all memory cells are in the specified states.
	
	\item \textbf{Instructions $I$}\\
	A list of reversible \textit{instructions} which are performed in sequence. These instructions have no immediately observable effects, such that they can be reverted in the event of a \textit{rollback}. Concretely, each instruction is one of:
	\begin{itemize}
		\item $check(p)$\\
		Trigger a rollback if predicate $p$ over data is satisfied.
		\item $fill_P(m, p)$\\Fill an empty memory variable $m$ with the result of a predicate $p$ over available data. It's data type is implicitly \textit{boolean}.
		\item $fill_F(m, f, a)$\\
		Fill an empty memory variable $m$ with the result of invoking function $f$ with parameters $a$, a list of references to data variables with length matching the arity of $f$. It is incorrect for $f$ to \textit{mutate} its arguments, as this would result in observable effects which cannot be rolled back.
		\item $swap(m_0,m_1)$\\
		Swap\footnote{In principle, any reversible data-agnostic manipulation is possible, but swapping values is sufficiently expressive and intuitive for our purposes.} the values in two memory variables~$m_0$ and~$m_1$.
	\end{itemize}
	If a rollback is triggered by $check$, any swapped memory cells are swapped back, and any memory cells whose values were created by $fill_P$ or $fill_F$ are destroyed.
	
	\item \textbf{Movements $M$}\\
	A mapping from \textit{resources} to any identifiers that can act as getters (getter ports and empty memory cells). This represents the \textit{observable effects} of the rule firing after instructions are performed without triggering rollback. 
\end{enumerate}

As an example to demonstrate this representation, the RBA rule in the previous section with data constraint $f(P_0)=f(P_1)$
and synchronization constraint $\{P_0, P_1\}$ can be represented in the imperative-form rule with:

\begin{tabular}{r|l}
rule element	&  value \\ \hline
premise	&  $(\{P_0, P_1\}, \{\}, \{t_0,t_1\})$ \\
instructions	& $[fill_F(t_0, f, [P_0]), \quad{} fill_F(t_1, f, [P_1]), \quad check(t_0 = t_1)]$ \\
movements	& $\{\}$ 
\end{tabular}
\vspace{1em}

For our purposes, omitting a source identifier $s$ from~$M$ can be interpreted as $s\rightarrow{}\{\}$ by default, understood to mean `discard $s$'. For other applications, it may be preferable to require that each source have an explicitly-defined destination set. For example, this would be useful in the context of a \textit{linear type system} in which data cannot trivially be \textit{destroyed}. 



\section{Code Generation Pipeline}
In this section we explore the translation procedure of protocols, starting from their declarative Reo specification in the Reo compiler's internal representation to executable Rust. This is done in two distinct phases, as explained in Section~\ref{sec:decoupling_reo_rust}. Section~\ref{sec:translation_phase_1} performs translation sub-tasks $T_{act}$ and $T_{type}$, and Section~\ref{sec:translation_phase_2} performs the final sub-task: $T_{run}$.

\subsection{Phase 1: Reo Compiler Backend}
\label{sec:translation_phase_1}
The Rust backend of the Reo compiler translates from the compiler's intermediate representation to Rust. Section~\ref{sec:decoupling_reo_rust} explains how this task is partitioned such that the compiler itself only performs the first of the two phases that comprise this process. Rust is still the target, but rather than generating the protocol object to be executed directly, Reo outputs a source file that contains a \code{ProtoDef}, the Rust-syntax version of a protocol in \textit{imperative form}. 


\subsubsection{Action Sequencing}
The most obvious role the backend plays in the translation process is to spread the contents of the relational, interaction-oriented internal representation over the imperative, action-oriented \textit{imperative rules}. This task is called $T_{act}$ in the previous sections. The process begins given (1) the set of port putters in the synchronization set, (2) a mapping from port-getter to \code{Term}, (3) a mapping from port-putter to \code{Term}, and (4) a \code{Formula}, representing the rule's guard. Terms and formulas are recursive data structures, and can both be thought of as \textit{sum types}. The former describes a an expression to be computed at runtime. For example, \code{MemoryCell(A)} and~\code{True} are both terms that can be assigned to memory cells and retrieved by getter-ports. \code{Formula} is similar, but used in the context of predicates; formulas are essentially terms known to have a boolean data-type, and thus come with variants for the usual boolean operations. For example, \code{True}, \code{And(Eq(A,B), Eq(B,C))} are examples of formulas. The backend traverses the compiler's internal representation, constructing imperative form rules one at a time, also populating the associated \code{NameDef} symbol table. For each such rule, it helps to consider each of its distinct structures in isolation, initially without any optimizations:

\begin{enumerate}
	\item [$P$] \textbf{Premise}\\

The first task of the backend is to build the imperative rule's \textit{premise}. Observe that the this step is nearly trivial, as the synchronization set, set of port-getters, and set of port-putters are already provided. The only exception is for memory cells which are \textit{read}, which must be derived by encountering their occurrences inside terms and formulas for the guard and assignments. Clearly, no information is added by explicitly isolating the set of readable memory cells; we are able to derive it despite being absent in the internal representation, and the term is only meaningful if one cannot read from a memory cell \textit{unless} it is marked as readable. We nevertheless perform this preprocessing as it makes explicit that the memory cell must be available for the interaction without having to `look ahead' into terms and formulas. Furthermore, its inclusion provides us a simplifying property: if an identifier does not occur in the premise, it must refer to a \textit{temporary variable}.

\item [$I$] \textbf{Instructions}\\
Next, the backend generates a list of \textit{instructions}, representing the \textit{tentative actions} that may be rolled back if the rule aborts instead of committing. Here, $fill_P$ and $fill_F$ instructions are inserted to compute the results of any evaluations of predicates or functions and bind them to temporary variables. Next, special cases of each memory cell~$M$ synchronously read-from and written-to are detected. For each, a temporary variable $T_M$ is created, and an instruction $swap(M, T_M)$ is appended to~$I$. As a result, data movements can be conducted in parallel without fear of a race condition emerging; although $M$ is logically involved both in reading and writing, these actions are distinguished by name; $M$ is as independent from $T_M$ as any two distinct memory cells. $M$ receives and stores its new (written) value $M$, and its old value can concurrently be read from $T_M$ without interference. Finally, a \textit{check} instruction is appended to the list, parameterized with an analog to the compiler's \code{Formula} type. 

\item [$M$] \textbf{Movements}\\
All port-getters and memory cells which are \textit{written to} are grouped by source to produce~$M$, a mapping from sources to destination sets. Each mapping is additionally annotated with a flag to indicate whether the value is retained at the source, ie.\ the mapping is of type $Identifier \Mapsto{} (boolean, \{Identifier\})$. As an example, moving a value from~$A$ to~$B$ results in $A$ mapping to $(\text{\textit{false}}, \{B\})$.

\end{enumerate}
Clearly, many opportunities exist for optimization of the imperative form, as it affords some degree of \textit{over}-specification and can represent the same connector with a different sequence of instructions. As an example, it is possible to fragment our guard into several \textit{check} instructions. This can be used to achieve \textit{short-circuiting} behavior; as long as no guard reasons about a temporary variable \textit{prior} to its definition, a check may be moved earlier such that rollbacks may be triggered earlier and work can be avoided. These opportunities were not explored extensively in this work, as instructions in practice tend to be simple enough not to afford many such opportunities.
More significant is the \textit{omission} of trivial checks entirely. This can be done by traversing instructions with a model of the protocol's state, initialized to match the premise. Checks known to always hold can be omitted, and checks known to never hold can result in the rule being discarded entirely (it would never fire!). This optimization is incorporated, and works to great effect in practice.


\subsubsection{Type Classification and Constraining}
Previously referred to as $T_{type}$, the backend must extract data-type information about its ports and memory cells from the compiler's internal representation such that the generated Rust is valid. Our approach is to begin by assuming that every port, memory cell and temporary variable has its own unrelated type. The types are then \textit{constrained} as a result of discovering the ways in which they are related from the protocol definition. For example, the presence of a term \code{Eq(A,B)} `collapses' the types of~$A$ and~$B$ into one equivalence class. Still, at this level, types are purely symbolic. 

These symbolic types exist only in the first phase of the code generator. They are not only resolved prior to phase two, but they are not present in that phase at all. Our imperative form does not deal with the complexity of symbolic (ie.\ generic or parametric) types. Instead, the Reo compiler relies on the Rust idiom of relying on \textit{dispatch} to resolve concrete types at the last possible moment: at the call site. To achieve this result, Reo generates the \code{ProtoDef} object wrapped in a \textit{wrapper function}, whose role is primarily to expose generic type parameters for instantiation to the caller, and construct a \code{ProtoDef} instance within the body, to be invoked with concrete types. Listing~\ref{listing:type_resolve} gives an example of how this generic function appears to the end user for some trivial protocol. Observe how \code{protocol\_1} relies on generics to let the caller, \code{main\_1} which type to select for~\code{T}. \code{TypeInfo} is a way of representing our symbolic type as data, allowing for the second phase to check for equality and so forth. The details of this type are provided in Section~\ref{sec:type_reflection}.


\begin{listing}[ht]
	\centering
	\inputminted[]{rust}{type_resolve.rs}
	\caption[TODO.]{TODO.}
	\label{listing:type_resolve}
\end{listing}

Unlike Java, Rust makes very few promises about the properties of some generic type. It cannot be certain that instances of some generic type~\code{T} will have a defined operation for checking equality, or for safe \textit{value replication}. To facilitate \textit{useful} polymorphism, Rust relies on \textit{type constraints} to act as a contract between caller and callee: the caller will ensure that only types which satisfy the bound may be selected for the type parameter, and the callee is able to interact with its generic arguments in accordance with \textit{behavior} the bounds guarantee the type will define. This approach may be familiar to programmers of Java or C, where this concept manifests as interfaces and declarations (usually in header files) respectively. 

The Reo backend cannot guarantee the generated Rust code is sound unless it is careful to add the necessary type bounds to its generic arguments. The internal representation is inspected for cases which will necessitate the use of specialized operations (such as replication) and will annotate its generic types with \textit{trait bounds}. Ultimately, the resulting coalesced, bounded type parameters are reflected in the generated code as part of the function declaration. The second phase of the code generator can rest assured that for every specialized operation inserted, Reo will have already anticipated the need to guarantee the bound is satisfied. Listing~\ref{listing:type_resolve2} gives an example of a generated trait bound for the case where two memory cells are related by being the same, and requiring the definition of an equality operation.

\begin{listing}[ht]
	\centering
	\inputminted[]{rust}{type_resolve2.rs}
	\caption[TODO.]{TODO.}
	\label{listing:type_resolve2}
\end{listing}


\subsubsection{Initial Protocol State}
The initial state of a protocol object is unusual in that it is included in the textual Reo protocol definition, but omitted from the \code{ProtoDef}. This design choice reflects how a protocol's initial state is unassuming at first glance, but is significantly different from the rest of the protocols definition: it is the only part of specification that cannot generally be replicated. Rules and name definitions describe \textit{behavior}, and do not involve any elements of the port's data domain directly. By contrast, a protocol's initial state does not \textit{describe} data, it \textit{is} data. By extracting this facet of the specification, \code{ProtoDef} becomes pure in its role as a \textit{blueprint} for a protocol's behavior. 

Nevertheless, textual Reo is able to specify the initial states of memory cells. To support this functionality, the Reo compiler itself generates code which \textit{builds and injects} the initial values for any initially-filled cells in a controlled environment. To mirror the textual descriptions that define the initial values, these memory types acquire a final trait bound such that their initial values can be constructed per protocol object instantiation \textit{within} the confines of the function that the Reo compiler generates. Listing~\ref{listing:mem_init} exemplifies a trivial protocol where~$A$ is a memory cell defined as being initialized by some value represented by the string `\texttt{hello}'. The generated code inserts a trait dependency, ensuring that the type parameter~\code{T} defines this operation. Observe how now the return result is no longer~\code{ProtoDef}, but rather~\code{ProtoHandle}. While the former represents a re-usable \textit{blueprint} for instantiating a~\code{ProtoHandle}, the latter represents an instantiated protocol, initialized and ready to run; the only user-facing means of generating a new object of the same protocol is to again invoke~\code{protocol}.

\begin{listing}[ht]
	\centering
	\inputminted[]{rust}{mem_init.rs}
	\caption[TODO.]{TODO.}
	\label{listing:mem_init}
\end{listing}


\subsection{Phase 2: Rust Library}
\label{sec:translation_phase_2}

\subsubsection{Soundness Check}

\subsubsection{Data Layout Optimization}
\subsubsection{Meta-State Initialization}